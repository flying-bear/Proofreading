\documentclass[fontsize=12pt,a4paper,twoside,openany]{scrbook}
\usepackage{clba}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[german,english]{babel}

% to link the references
\usepackage{hyperref}
\hypersetup{colorlinks=true,
            linkcolor=blue,
            filecolor=magenta,      
            urlcolor=cyan,
            citecolor=[rgb]{0,0.4,0},
            pdfpagemode=FullScreen,}

% listing json file
\usepackage{listings}
\usepackage{minted}
\usepackage{xcolor}

% two figures side by side
% \usepackage{subcaption}

% break lines in table
% \usepackage{makecell}
\usepackage{tabularx}
% \newcolumntype{Y}{>{\centering\arraybackslash}X}

% to use list of abbreviations
% \usepackage[acronym]{glossaries}

% not to be hyphenated
\hyphenation{HDBSCAN}

% add blank line between paragraphs
\setlength{\parskip}{1em}

% Import the biblatex package and sets a bibliography style
\usepackage[backend=biber, style=authoryear,parencitestyle=authoryear,bibstyle=numeric]{biblatex}
\addbibresource{bibfile.bib} %Imports bibliography file

% Per Kapitel Nummerierung von Graphiken und Tabellen
\counterwithin{figure}{chapter}
\counterwithin{table}{chapter}

% Hier die eigenen Daten eintragen
\global\fach{Computational Linguistics }
\global\arbeit{Master's Thesis}
\global\titel{The Validation of Topic Clustering based on Top2Vec Topic Modeling Method}
\global\author{Nataliia Sherstneva}
\global\betreuer{M.Sc. Lütfi Kerem Şenel}
\global\pruefer{Prof. Dr. Hinrich Schütze}
\global\universitaet{Ludwig-Maximilians-Universität München}
\global\fakultaet{Faculty of Languages and Literatures}
\global\department{Department 2}

\global\abgabetermin{26 July 2021}
\global\bearbeitungszeit{12 March - 04 July 2021}
\global\ort{Munich}

\begin{document}

% Deckblatt
\deckblatt
\pagestyle{scrheadings}
\pagenumbering{gobble}

% Erklärung fürs Prüfungsamt
\erklaerung

% Zusammenfassung
\addchap{Abstract}
\pagenumbering{Roman}
\thispagestyle{scrplain}
\noindent
Dieses Dokument dient als Muster für eine Ausarbeitung einer
Bachelorarbeit am CIS und wird in deutscher oder englischer Sprache
erstellt (hier max. 250 Wörter)

% Acknowledgements
\addchap{Acknowledgements}
\thispagestyle{scrplain}
I would like to thank...

% Inhaltsverzeichnis
\tableofcontents

\chapter{Introduction}
In the past several decades...

% Text mit arabischer Nummerierung
\pagenumbering{arabic}

\section{Motivation}
Here comes Motivation...

\section{Contribution}

In particular no study, to our knowledge (to the best of my knowledge), has considered ....


no previous research has investigated

There is no previous research using ... approach.

\section{Outline}
The rest of the thesis is organized as follows.


\chapter{Related Work and Theoretical Framework}
\label{chapter:theory}
In recent years, computer science has greatly advanced in deep learning architectures which enable the use of new approaches and techniques for data and text analysis \parencite{Minaee20} and which combine such methods to achieve better performance. Since the main topic of the present paper, Top2Vec algorithm, utilises several such techniques for searching a topic cluster in semantic space, this chapter aims to give an overview of all the relevant methods. 

The rest of the chapter is organized as follows. In Section \ref{sec:A} such state-of-the-art deep learning embedding techniques as Paragraph Vector or \emph{doc2vec} (Section \ref{sec:doc2vec}) and transformer-based sentence embedding (Section \ref{sec:sbert}) are presented. An overview of various topic modelling approaches is provided in section \ref{sec:B}. Section \ref{sec:C} is dedicated to density based clustering, specifically HDBSCAN algorithm (section \ref{sec:HDBSCAN}) and clustering validation methods (\ref{sec:validation}).  

\section{Embedding techniques in text analysis}
\label{sec:A}

The introduction of embeddings was an important step in machine learning that began a new era in natural language processing. Before embeddings, the numeric representation of texts was acquired using simple approaches like one-hot encoding and the Bag-of-Words (BOW) method. These techniques, however, have some weaknesses. Among them, two drawbacks that are most crucial for text representation should be mentioned. Firstly, the lack of consideration for word meaning in these methods leads to equal distance between similar and dissimilar words in the semantic spaces. Secondly, both methods disregard word order, and as a result, semantically different sentences consisting of the same words might have the same vector representation. The introduction of methods enabling learning of distributed representation of words, rendered it possible to capture syntactic and semantic relationships between the words \parencite{Le14}. 

The first method aimed at learning dense vector representation of words with neural networks, known as the \emph{word2vec} algorithm, was introduced by Tomas Mikolov \parencite{TMikolov13} (see also \parencite{Mikolov13}). It is based on the so-called "distributional hypothesis" \parencite{Harris54}. The main idea of the hypothesis was expressed by the English linguist John Rupert Firth, who said that a word is characterized by the company it keeps \parencite{Firth57}.

Having a numerical representation for separate words in a document naturally raises the question of whether it is possible to create continuous distributed vector representations for larger pieces of texts like a sentence, paragraph, or even an entire document, regardless of its length. So, in 2014 Mikolov and Le proposed Paragraph Vector Framework also known as \emph{doc2vec} which was an extension of the word2vec algorithm \parencite[see][]{Le14}.

Another state-of-the-art approach to sentence embeddings uses Transformers. There are a lot of methods based on Transformer architecture which is attempts to obtain embedding for a larger block of the text, i.e. sentences. The most common of the Transformer-based methods are: Skip-Thought Vectors, which provide generic sentence representations \parencite{Kiros15}, and their variation, FastSent \parencite{Hill16}, and several supervised sentence embedding techniques, namely, InferSent \parencite{Conneau17}, proposed by Facebook AI Research in 2018, Universal Sentence Encoder, and Sentence BERT.

Due to time restrictions, not every embedding model could be tested in the present study. Thus, in the next sections, only the most relevant methods of getting numeric texts representation are considered. Firstly, word2vec embedding model is covered briefly and then doc2vec algorithm, which is based on it, is discussed in more detail (Section  \ref{sec:doc2vec}). Among Transformer-based methods, Universal Sentence Encoder (Section \ref{sec:muse}) and Sentence-BERT (Section \ref{sec:sbert}) are considered. Both Transformer-based models are used in Top2Vec topic modelling and, according to the previous experiments, yield the best performance on the downstream tasks \parencite{Wang20}.

\subsection{Paragraph Vectors Framework (doc2vec)}
\label{sec:doc2vec}

Word2vec model uses a simple feed-forward neural network to calculate a vector for each word by the concatenation or averaging it with other word vectors in a context ("window"). As a result, each word is mapped to a continuous lower-dimensional vector. The semantic similarity is captured by the distance in the resulting vector space. In other words, the word-vectors of similar words are close to each other and the vectors of dissimilar ones are distant from each other.

The word embedding model for learning word vectors consists of two frameworks: the continuous Bag-of-Words (CBOW) model and the continuous Skip-gram model. Their architectures are depicted in Figure \ref{fig:cbow_skip_gr}. The first algorithm, CBOW, tries to predict a target word based on its surrounding context words. The goal of the second, continuous Skip-gram model, is to predict the context words based on a target word.

\begin{figure}[h]
\centering
\includegraphics[width=0.90\textwidth]{cbow_skip_gr.png}
\caption{The CBOW and Skip-gram architectures from \parencite{TMikolov13}}
\label{fig:cbow_skip_gr}
\end{figure}

The architecture of Paragraph Vector Framework (\emph{doc2vec}) is based on the previous method for learning word vectors, the only difference being that the prediction of the next word uses an additional document-unique feature vector alongside the word-vectors. This way, each paragraph (or a larger text) is mapped to this additional vector represented by a column in matrix \emph{D}, while words are still represented by a column in matrix \emph{W}, just like in the word2vec algorithm. 

There are two implementations of the doc2vec Model: the Distributed Memory Model of Paragraph Vector (PV-DM) and Distributed Bag of Words version of Paragraph Vector (PV-DBOW).

In the Distributed Memory Model, like in word2vec CBOW, every paragraph or document is mapped to a unique feature vector as well as every word. For the prediction task, the paragraph vector is concatenated or averaged with the context word vectors (see Figure \ref{fig:pv-dm}). Here, the paragraph vector acts as a memory of the paragraph's or document's topic, since it contains the missing information from the current context. After training, this vector is used as a feature for paragraph or documents to make a prediction.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.70\textwidth]{pv-dm}
\caption{The architecture of PV-DM Model for an input sentence}
\label{fig:pv-dm}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.70\textwidth]{pv-dbow}
\caption{A distributed Bag of Words model of Paragraph vectors}
\label{fig:pv-dbow}
\end{figure}

The other Paragraph Vectors framework is similar to the word2vec Skip-gram model. The paragraph vector is here simplified, and no local context is used. The goal of this model is to predict the context words, randomly sampled from the paragraph vector \parencite{Le14}, as shown in Figure \ref{fig:pv-dbow}. Despite the simplicity of this model, it has a better performance for the topic modelling task \parencite{Angelov20}.

The Paragraph Vectors Model has several important advantages. First of all, Paragraph Vectors capture the semantics of the words and even document semantics in dense vectors, so that the similar words (or documents) get the similar vectors and are located closer to each other in the vector space. Secondly, the Paragraph Vectors Model can be applied to the text of any length and does not require a task-specific tuning. Thirdly, the word order is taken into account at least in a small context \parencite{Dai15}. And finally, no labelled data are needed for the training process. 

All of that makes Paragraph Vectors Framework very promising for various NLP tasks, such as similarity search, text classification, and topic modelling. Experiments show that doc2vec model usually outperforms not only Bag-of-Words model but also the word2vec one \parencite[see][]{Lau16, Le14}.

\subsection{Universal Sentence Encoder}
\label{sec:muse}

The Universal Sentence Encoder was developed by Google and presented in \parencite{Cer18a} and \parencite{Cer18b}. The concept of the Universal Sentence Encoder (USE) is actually an umbrella term for a family of TensorFlow sentence encoding models. Currently, it includes 11 models based on multitask training, and hence their sentence embeddings (or high-dimensional vectors) can be used for such tasks as text classification, clustering,  fine-grained question-answer retrieval, semantic similarity search, etc.\footnote{Collection of universal sentence encoders. \url{https://tfhub.dev/google/collections/universal-sentence-encoder/1}}

The USE method consists of two encoding models for sentence representation learning: one of them is based on the transformer encoder architecture, while the other one uses the Deep Averaging Network (DAN) model \parencite{Cer18a} (see Figure \ref{fig:use_architecture}).

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]{use_architecture.png}
\caption{The two models of the Universal Sentence Encoder: (a) Transformer Encoder and (b) DAN}
\label{fig:use_architecture}
\end{figure}

The architecture of the first model consists only of the encoder part of the original transformer architecture, presented in \parencite{Vaswani17}. Not using a Decoder part allows to significantly reduce the training time, while keeping the transfer task performance. The USE transformer architecture consists of six stacked transformer encoder layers, where each of them has a self-attention layer followed by a feed-forward network. Generating a word embedding with self-attention takes the word order and the surrounding context into account \parencite{Cer18a}. Then the context-aware embedding representation of the sentence words is averaged together to get an output, which is a sentence level 512-dimensional embedding vector. 

The advantage of this model is that it has good accuracy on downstream tasks. However, since the self-attention has \(O(n^2)\) time complexity, the time of computation scales significantly if the length of the sentence increase.

The second variant of USE is based on the Deep Averaging Network (DAN) \parencite{Iyyer15}. In this model, the embedding of uni-grams and bi-grams are computed and averaged into a single embedding. The averaged embedding is then fed into a deep feed-forward neural network, yielding a final sentence embedding, again a 512-dimensional vector.

In contrast to the Transformer variant, the DAN encoder has more efficient inference time, since it does not have any self-attention layers and uses only feed-forward operations, the time complexity of which is linear and equal to the length of the input sentence. However, this USE model has lower accuracy than the transformer USE variant. 

Most pre-trained sentence embedding models are language-specific. To be able to process an input of a particular language and to produce an embedding for it, the model should be trained for this language. The crucial difference of a multilingual USE is that it is trained simultaneously on multiple tasks across languages. The deep learning technique that enables multilingual sentence embeddings is called "Multi-task Dual-Encoder Model" \parencite{Chidambaram19}. This allows to compare the sentences from different languages, since the embeddings of texts in different languages with similar meaning are closer to each other than the embeddings of dissimilar texts. 
Testing the multilingual USE as a pre-trained embedding model for the Top2Vec algorithm was performed as a part of this thesis (see Section \ref{sec:muse_results}).

\subsection{Sentence-BERT}
\label{sec:sbert}

Until recently, BERT was one of the common methods to get numerical sentence representation \parencite{Devlin19}. Different layers of this pre-trained transformer network learn different linguistic properties, and thus it is possible to get a sentence representation by merging information obtained from these different layers and either averaging the output layer across tokens or using an output of the first [CLS] token \parencite{Wang20}. However, the quality of embedding resulting from this method was not very high and due to the very large model size the complexity of finding the most similar sentence pair was also very high (ab. 65 hours) \parencite{Reimers19}. These limitations make BERT not suitable for working with a large corpora.

New state-of-the-art algorithm in sentence embedding is called Sentence-BERT \parencite{Reimers19}. As the name implies, it is also based on BERT and transformer architecture. Like this pre-trained network, Sentence-BERT also makes use of the attention mechanism, which allows the model to create an embedding by focusing only on the most important parts of the text input. Yet, Sentence-BERT modifies the architecture of BERT (or its extensions like RoBERTa) by adding a pooling operation to its output \parencite{Reimers19}.

The crucial point of Sentence-BERT's architecture is the usage of the so-called siamese and triplet network. As shown in Figure \ref{fig:S-BERT-arch}, Sentence-BERT takes two sentences from the pre-trained BERT model and then uses the polling layer to generate the embedding for each sentence. Then they are concatenated and a softmax function is applied. For the classification task, the embeddings of two sentences \emph{u} and \emph{v} is used as an input to compute the cosine similarity score.


\begin{figure}[h]
\centering
\includegraphics[width=0.90\textwidth]{S-BERT-arch.png}
\caption{SBERT architecture with classification objective function for fine-tuning (left) and at inference with regression objective function (right) \parencite[from][]{Reimers19}}
\label{fig:S-BERT-arch}
\end{figure}

These modifications make Sentence-Bert suitable for such tasks as clustering or semantic similarity search, for which BERT is unsuitable. The evaluation of this state-of-the-art algorithm for the common Semantic Textual Similarity (STS) task shows that it not only has higher computational efficiency than other state-of-the-art sentence embedding methods like InferSent of Universal Sentence Encoder, but also outperforms them.

To obtain a multilingual and cross-linguistic sentence embedding, SBERT model was extended to other languages through the use of the multilingual knowledge distillation method \parencite{Reimers20}. Within this approach, one model is treated as a student model and another one as a teacher model. Based on parallel sentence data, from the teacher model the student model adopts a multilingual semantic embedding space for the original source language, which is usually English, and then transfers it to other languages \parencite{Reimers20}. 

\section{Topic modelling}
\label{sec:B}

A numeric representation of sentences or larger text fragments is a prerequisite to finding their semantic similarity. However, when working with a large corpus, it is often also desirable to derive specific topics that occur in it or topic clusters. This is one of the common and well-developed tasks in modern computational linguistics. Thus, this section of the paper is dedicated to topic modelling, which is one of the text analysis methods that enable automatic extraction of topics present in a corpus.

Topic modelling belongs to the unsupervised machine learning techniques, which means that discovering semantic structures in a text and identifying the topics present in a corpus does not require any prior knowledge about the data and true classification of the documents in the corpus. Topic modelling is not the only technique used for identifying topics within the texts via analysing word groups. Some other methods, like cluster analysis or latent semantic analysis (LSA) \parencite[see][]{Foltz96, Landauer2007} are also aimed at it. In all these methods, the dimensionality reduction of the full document's representation is performed to join the documents into groups \parencite{Anandarajan18}. 

In both LSA and topic modelling methods, the document-term matrix (DTM) or term-document matrix (TDM) is used as the input for the analysis. The difference is that, in contrast to LSA, the aim of topic modelling is not to reveal a hidden semantic structure of the document, but to discover a thematic one. Topic modelling concentrates on the underlying themes that appear in a document collection and determines the probability that each document is associated with a given topic. 

There are two types of topic models: mixed-membership and single-membership models. In the first type of modelling, each document belongs to several topics with a different probability distribution, in the second one, each document can be assigned only one topic. Latent Dirichlet Allocation (LDA) model is an exaple of the mixed-membership topic modelling, while the Top2Vec topic modelling algorithm represents the single-membership models. In the next two subsections, these techniques are covered in more detail.

\subsection{Probabilistic topic modelling}
\label{sec:lda}

In the probabilistic topic modelling, the data are viewed as arising from a generative process \parencite{Blei12}. Each document is understood as a combination of topics and every topic is seen as a probability distribution of words within it. 

The most common form of probabilistic topic modelling is latent Dirichlet allocation introduced in \parencite{Blei03}. LDA is based on the bag-of-words assumption, which means the word order is disregarded when building a topic model. Thus, each word that appears in the corpus is randomly assigned to one of the topics with certain probability scores as illustrated in Figure \ref{fig:lda_intuition}. Assuming Dirichlet prior distribution for both document-topic and topic-word distributions, LDA treats any topic as a distribution over words from the corpus and each document as a collection of corpus-wide topics \parencite{Blei03}.

\begin{figure}[h]
\centering
\includegraphics[width=0.80\textwidth]{lda_intuition.png}
\caption{The intuition behind the LDA algorithm. From \parencite{Blei12}}
\label{fig:lda_intuition}
\end{figure}

LDA topic model has several input parameters:
\begin{itemize}
  \item \(K\) is the total number of topics that should be extracted from all the documents in the corpus;
  \item \(D\) is the total number of documents;
  \item \(N\) denotes the total number of words in a document \emph{d}, where W\(_{d,n}\) is an observed n\(_{th}\) word in that document.
  \item \(\alpha\) is a Dirichlet parameter which denotes the density of per-document topic distribution,
  \item \(\beta\) is another Dirichlet parameter which is responsible for per-topic word distribution,
  \item \(\eta\) is a Dirichlet topic hyperparameter.
\end{itemize}

All these parameters are represented in Figure \ref{fig:lda} that is a plate representation of the generative process assumed in LDA. The biggest outer plate should be considered as a corpus of documents, while the inner plate stands for the repeated choice of topics \(Z\) and words \(W\) within a document \(N\) \parencite[see also][p.~997]{Blei12}. Each document \emph{d} is seen as a combination of topics with topic proportions \(\theta_{d}\) coming from Dirichlet(\(\alpha\)). Each topic \(k\) has a distribution over words \(\beta_{k}\) from Dirichlet(\(\eta\)). In each document, each word W\(_{d,n}\) is drawn from a topic \(Z_{d,n}\) with a topic assignment \(\beta_{k}\). 

\begin{figure}[h]
\centering
\includegraphics[width=0.70\textwidth]{lda.png}
\caption{Graphical representation of the random variables (nodes) in the LDA model with their dependencies (edges).}
\label{fig:lda}
\end{figure}

Informally, in LDA topic modelling, every term in a document can come from a different topic or can be seen as belonging to different topics with a different probability score. Every document can be assigned to several topics also with a different probability score.

Although LDA is one of the most widely used topic modelling algorithms, it has some limitations. First, as the model is based on the bag-of-words representation of documents, it does not preserve the meaning coming from word order and thus ignores word semantics. To address this problem, techniques like stemming and lemmatization are used as pre-processing. However, that does not help to highlight the similarity of words with different word stems (like \emph{huge} and \emph{big}). Second, LDA assumes that the number of topics presented in the corpus is known in advance and is required by the algorithm as a hyperparameter (\(K\)). Unfortunately, the quality of extracted topics clusters strongly depends on this value, which, in most cases, is unknown and usually cannot be chosen intuitively, thus requiring a lot of time-consuming tests. 

LDA is only one example of probabilistic topic modelling. Within this paradigm, there are other approaches, such as correlated topic model (CTM) \parencite{Blei05}, dynamic topic model (DTM) \parencite{Blei06}, structural topic model (STM) \parencite{Roberts13}, etc. All of these approaches use LDA as the basis for the model with some extensions.

\subsection{Topic modelling with Top2Vec}
\label{sec:Top2Vec}

Top2Vec, proposed by Angelov in \cite*{Angelov20}, is a new unsupervised algorithm for topic modelling and semantic search. In contrast to LDA, it does not require any pre-processing like removing stop-words, stemming, or lemmatization, and can automatically detect the number of topics present in a large corpus. Moreover, the Top2Vec algorithm uses semantic embedding instead of the Bag-of-Words representation, improving the ability of the model to capture word similarity.

To identify the topic clusters in a collection of documents, Top2Vec first converts all document to high-dimensional embedding vectors. In this semantic vector space representations of similar documents are located closer to each other, forming dense areas of document vectors, while dissimilar ones are located further apart from each other creating sparse areas. Then Top2Vec identifies dense clusters of document vectors and to determines what topic each cluster is about.

Top2Vec is based on three algorithms: DBOW, HDBSCAN, and UMAP. To obtain semantic embedding, Top2Vec utilizes Paragraph Vectors or doc2vec model (see Subsection \ref{sec:doc2vec}). In particular, it uses Distributed Bag-of-Words Model (DBOW) to learn jointly embedded document and words vectors. Therefore, in the generated semantic vector space the distance between similar documents is smaller than between dissimilar ones. Since word and document are embedded jointly, each document (purple points in Figure \ref{fig:sem_space}) is surrounded by its most representative words (green points in the same figure).  

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]{sem_space.png}
\caption{The illustration of semantic space from \parencite{Angelov20}}
\label{fig:sem_space}
\end{figure}

This semantic space, which consists of words and document vectors, is treated in Top2Vec as a \emph{"continuous representation of topics"} \parencite[p.~3]{Angelov20}. Accordingly, the areas with a large concentration of high-dimensional documents vectors, the dense areas, contain highly similar documents. The Top2Vec algorithm supposes that the number of these dense areas is equivalent to the number of underlying topics in the corpus. 

The next step in the topic modeling process is identifying these dense areas of documents in the vector space obtained with DBOW. For that purpose, the HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) \parencite{McInnes17b, McInnes17a} algorithm is used.\footnote{In more detail the HDBSCAN algorithm is addressed in the following subsection \ref{sec:HDBSCAN}.} However, since HDBSCAN requires low-dimensional data and the output of DBOW is high-dimensional document vectors, a dimensionality reduction using UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) algorithm \parencite{McInnes18, McInnes20} is performed.

\begin{figure}[h]
\centering
\includegraphics[width=0.90\textwidth]{umap-hdbscan.png}
\caption{UMAP dimensionality reduction and density based clustering \parencite[see also][]{Angelov20}}
\label{fig:umap-hdbscan}
\end{figure}

When dense areas with jointly embedded document and word vectors are found the topics are determined by calculating the topic vectors of each cluster as an arithmetic mean of all documents vectors from the corresponding cluster. The word vectors that are closest to the resulting topic vector are considered to be the words representative of this topic. Furthermore, despite the absence of pre-processing, like the removal of stop-words, the original paper claims that topics keywords do not contain any of them. This is due to the fact that such words are so frequent, that they appear in most documents and therefore in the semantic space they are equidistant from all of them.

\begin{figure}[h]
\centering
\includegraphics[width=0.80\textwidth]{topics.png}
\caption{Examples of some topic clusters found by Top2Vec algorithm}
\label{fig:topics}
\end{figure}

Additionally, similar topics are usually placed close to each other in the resulting semantic space. For example, in Figure \ref{fig:topics} topic 14 that deals with the Astrazeneca vaccine and its side-effects and topic 72 that concerns other COVID-19 vaccines are located close to each other. The same effect can be observed on topics 29 and 57, that both concern elections. Such topics can be grouped together with hierarchical topic reduction.


\section{Density-based clustering}
\label{sec:C}

This Section covers the density-based clustering algorithm HDBSCAN applied in Top2Vec. Since analysing high-dimensional vectors is susceptible to the so-called "curse of dimensionality" \parencite{Theodoridis08}, the dimensionality reduction of vector space is required before clustering. Thus, some dimension reduction techniques are also discussed in this section.

\subsection{Dimensionality reduction}
\label{sec:umap}

In a high-dimensional space, the vectors that constitute a cluster are very sparse. This leads to high computational complexity, which makes it difficult to find the clusters \parencite{Angelov20}. Hence, an important step before applying a clustering algorithm is to perform a dimensionality reduction algorithm and to eliminate unnecessary features.

There are several dimensionality reduction techniques. Linear dimensionality reduction methods include ones like Linear Discriminant Analysis (LDA) \parencite{Tharwat17}, Singular Value Decomposition (SVD) \parencite{Klema80} or SVD-based Principal Component Analysis (PCA) \parencite{Hotelling33, Jolliffe86}. However, real data are usually not linearly separable and therefore the methods mentioned above do not have a good performance on them. Such cases require non-linear or manifold learning techniques. 

One of such non-linear methods is Uniform Manifold Approximation and Projection (UMAP) that relies on the manifold theory and topological data analysis \parencite{McInnes18, McInnes20}. UMAP is believed to have advantages over another well-known method for dimensionality reduction, t-distributed Stochastic Neighbour Embedding (t-SNE) \parencite{Maaten08}. In contrast to t-SNE, UMAP method represents both local and global structure of the data and is scalable to larger data sets. 

UMAP has several hyperparameters, namely, the number of nearest neighbours \emph{n\_neighbors} that control to what extent local and global structure is preserved, a metric to compute the distance, and the parameter to determine the number of dimensions after the dimensionality reduction.

The dimensionality reduction with UMAP algorithm is computed in two steps. First, k nearest neighbours are extracted through building a weighted k-neighbour graph \parencite{McInnes20}, then a low dimensional layout of this graph is computed.

The t-SNE algorithm due to its sensitivity to the local structure is usually used for data visualization followed by a manual investigation\footnote{For these purposes a good data transparency in automatically detected clusters can be reached with such tools as pandas Python package and interactive data visualization library \href{https://bokeh.org/}{Bokeh}.} of the clusters. Here, the dimensionality reduction is performed in the following way: firstly, the similarity between data points in the high-dimensional space is transformed into a probability distribution. Then the difference between the distribution in the original high-dimensional space and that in the embedded 2D or 3D space is minimized. One of the main advantages of this method is that it can maintain the local structures and therefore is good at dealing with density-based clusters, that are arbitrarily shaped. However, this method also has several hyperparameters and, depending on their values, pays more attention either to local or to global structure.

\subsection{HDBSCAN for topic clustering}
\label{sec:HDBSCAN}

HDBSCAN method or Hierarchical-DBSCAN was proposed by \parencite{Campello13, Campello15} as an improved version of two algorithms: DBSCAN clustering method (or Density-Based Spatial Clustering of Applications with Noise \parencite{Ester96}) and its extension, OPTICS algorithm \parencite{Ankerst99}.

In DBSCAN, the identification of the dense regions depends on two input parameters:  \(\varepsilon\) (\emph{epsilon}) that determines the neighbourhood radius around any point \emph{x} and the minimum number of points (\(min_{pts}\)) within this neighbourhood. The point x\(_p\) is considered as a \emph{"core object"} \parencite{Campello13}, if within its \(\varepsilon\)-neighbourhood the number of points is equal or greater than \(min_{pts}\). The points that are located inside the \(\varepsilon\)-neighbourhood but do not have enough points to be a core object are called \emph{border points} (like the point \emph{y} in Figure \ref{fig:core_point}). Other points are treated as noise since they are neither core not border objects (like the green point \emph{z} in the same Figure). The main idea of DBSCAN is that the subset of all data points is considered a cluster if each point in this subset is directly or transitively density-reachable from some core point within this cluster \parencite{Ester96}.

\begin{figure}[h]
\centering
\includegraphics[width=0.70\textwidth]{core_point.png}
\caption{The classification of points in DBSCAN clustering algorithm.}
\label{fig:core_point}
\end{figure}

This method has some disadvantages. The result of the clustering mostly depends on the \emph{epsilon} parameter which is hard to choose intuitively and set correctly. It appears that this "\emph{global} density threshold" \parencite{Malzer20} prevents detecting clusters with different densities. Another problem of DBSCAN is that it is not deterministic, since if the border point is density-reachable from more than one cluster (core points) the result, to what cluster it will be assigned depends on the implementation and processing order of the data. 

HDBSCAN overcomes all these limitations and does not require any \(\varepsilon\)-threshold parameter. The minimum number of neighbours (\(min_{pts}\)) is the only input parameter that should be fed to the algorithm. Then the hierarchical structure ("HDBSCAN hierarchy" \parencite{Campello13}) w.r.t. \(min_{pts}\) will be built with all possible values for the \(\varepsilon\) neighbourhood radius. In order to identify noise points and to distinguish them from the objects in the dense area HDBSCAN's authors introduced the concept of \emph{"mutual reachability distance"} \parencite[p.~163]{Campello13} between two objects \(\textbf{x}_p\) and \(\textbf{x}_q\), that is defined as follows: \[d_{mreach}(\textbf{x}_p, \textbf{x}_q) = max\{d_{core}(\textbf{x}_p), d_{core}(\textbf{x}_q), d(\textbf{x}_p, \textbf{x}_q)\}\] 
where \(d(\textbf{x}_p, \textbf{x}_q)\) is a chosen distance (Euclidean, Manhattan, Jaccard, etc.) between these two points. It makes HDBSCAN more robust to noise and enables to abandon the concept of border points. 

Using Prim's algorithm \parencite{Mamun16} detected data subsets can be visualized in the form of the minimum spanning tree with data points as the vertices, connected by the edges according to the mutual reachability distance. Based on the minimum spanning tree as a next step the cluster hierarchy can be represented as a dendrogram by sorting the edges in the ascending order of the distance value. Then a different number of flat clusters with a number of points greater than or equal to \(min_{pts}\) can be obtained by setting the \(\varepsilon\)-threshold to the particular value.

One of the most advantages of HDBSCAN is that it is able to find arbitrary shaped clusters with diverse density and has more robustness to noise. These characteristics are of importance especially in the topic modelling context since in the semantic space the document vectors have a different density \parencite{Angelov20} and clusters to be identified are usually arbitrary shaped. With HDBSCAN it is possible to detect the number of clusters automatically without having any intuition about its shapes and density. The dense areas of document vectors will be assigned a positive label, while the objects located in the sparse areas will get a negative noise label (-1) \parencite{Angelov20}.

\section{Clustering validation techniques}
\label{sec:validation}
Since clustering belongs to the unsupervised machine learning techniques where there is no prior information about the underlying structure and data distribution, the validation procedure of the resulting clusters becomes important, if rather the quite challenging issue. All approaches to the cluster validity assessment are divided in the literature into two main classes based on external and internal criteria \parencite{Halkidi01a}. In the next two sections, the most relevant for this thesis validation measures are described.

\subsection{External clustering validity methods}
External indices are also treated as a supervised validation approach \parencite{Palacio19} since the validation procedure here is based on pre-existing knowledge like true data labels ("ground truth"). 
Among external measures for clustering validation, the most known one is the adjusted Rand Index \parencite{Hubert85}. It belongs to the binary external measures \parencite{Handl05} and based on the true and predicted labels for each item considers all sample pairs and computes the similarity between two sets of labels. The resulting values of the index lie within the range of -1.0 to 1.0 score, where the higher values mean a better clustering. The main advantage of the index is its interpretability and usability for all sort of clustering algorithms as long as true labels are provided. 

Another similar approach is represented by Jaccard similarity score where the intersection of two label sets divided by the number of all labels \parencite{Arnaboldi15}. However, since this metric rewards only positive label agreements \parencite{Handl05} it is not reliable enough in case of a lack of positive labels for some classes.

\subsection{Internal clustering validity methods}
Within this approach the quality of clusters is estimated relying only on the clustering information (assigned labels) and that of one inherent in the data itself. Here there are two main criteria the internal validation is based on:
\begin{itemize}
  \item \emph{Compactness} (cohesion) deals with the intra-cluster distance and estimates the closeness of the elements within each cluster.
  \item \emph{Separation} addresses the inter-cluster distance and indicates how well-separated each cluster is from other ones.
\end{itemize}
According to a definition a clustering algorithm is treated as a good one if it has a small intra-cluster distance and large inter-cluster distance \parencite{Kim05}. Therefore, most clustering validation methods take into account both criteria during the assessment. Among them, the most popular validation measures proposed in the literature are Calinski-Harabasz index (\emph{CH}) \parencite{Calinski74}, Dunn's index \parencite{Dunn74}, Davies-Bouldin index (\emph{DB}) \parencite{DaviesB79}, Silhouette index \parencite{Rousseeuw87} and some others. In order to find the optimal number of clusters the corresponding index should be maximized or as in the case of DB index minimized.

However, dealing with density-based clustering algorithms that identify arbitrary shaped clusters, these measures have no good performance since they estimate separation based on the cluster centroIDs and are not able to handle with noise in the data \parencite{Moulavi14}. As an attempt to handle with algorithms based on density and hierarchical analysis \textcite{Saracli13} tried to find the best number of clusters using as an evaluation metric a Cophenetic Correlation Coefficient (\emph{CPCC}). This index proposed by Sokal and Rohlf in \cite*{Sokal62} makes use of a dendrogram of the hierarchical clustering and shows the correlation between this dendrogram (or cophenetic matrix) with cophenetic distances and distance (proximity) matrix that contains the information about the similarity of observations. Although this index is quite appropriate for hierarchical clustering \parencite{Palacio19}, it still cannot capture well the arbitrary shaped clusters. Another drawback is its high computational complexity.

Halkidi and Vazirgiannis have introduced in \cite*{Halkidi01b} the \emph{S\_Dbw} index (Scat-Density between and within clusters index) that was intended to find the better partitions in the data by taking into consideration the density variations among clusters \parencite{Halkidi01b}. Comparing with other clustering validation measures, \emph{S\_Dbw} index performs very well for different clustering algorithms including hierarchical one and DBSCAN \parencite{Liu10}. Nevertheless, since the separation between clusters is computed based on the centres of the clusters (centroIDs) this index cannot contend with non-convex shapes of clusters.

In order to overcome this limitation, the same authors have proposed then another measure called \emph{CDbw} index (Composed density between and within cluster) \parencite{Halkidi08}. The crucial difference of \emph{CDbw} index to the former validation measures was that in this approach each cluster is represented not by a single point but by multi-representative points \parencite{Halkidi08}. Due to this, \emph{CDbw} index based on the density distribution of points within and between clusters can also handle aspherical arbitrarily shaped clusters. Nonetheless, this validity index is also not without shortcomings. The main drawback of this metric is that when taking the number of representative points for each cluster to evaluate it, \emph{CDbw} index keep this number of points unchanged during the whole validation process. As a consequence, the cluster with a different density or number of objects cannot be evaluated correctly or even under evaluation \parencite{Moulavi14}.

The new density-based clustering validation metric called DBCV was proposed by \textcite{Moulavi14} to overcome all drawbacks of the former measures by taking into account both density and shapes of the clusters. In DBCV the idea of the density computation is based on the Hartigan’s model of Density Contour Trees \parencite{Hartigan75}, within that clusters are defined as areas with high density separated from each other by low-dense areas \parencite{Moulavi14, Hartigan75}. Based on that, the authors of the DBCV index assume that the least dense regions inside each cluster are still denser than the most dense areas between them \parencite{Moulavi14}. Therefore, for the evaluation of the compactness of the clusters they compute the least dense regions inside each cluster and in order to measure between-cluster separation they evaluate the highest density areas between these clusters. This new definition of the object's core distance makes DBCV index more capable when dealing with noise in the data and enable it to capture the arbitrarily shaped clusters of different shapes, sizes and densities. The index output a score in the range [-1, 1], where the large index value means the better clustering.

\section{Summary}
Here considered in the sections above the most relevant for this thesis methods will be summarized. Firstly, different embedding techniques like doc2vec and transformer-based sentence embedding methods are reviewed. The Paragraph Vector Model (doc2vec) is based on the previous method using for learning word vectors (word2vec) but have integrated an additional document-unique feature vector also enables the learning of document vectors. The Transformer-based Sentence Embedding technique is represented by such models as InferSent, Universal Sentence Encoder and Sentence BERT. All of these models are based on the transformer architecture and differ in the way of learning of sentence numeric representations.

Topic modelling approaches include mixed-membership and single-membership models. The most well-known method among the first type of topic modelling is the generative probabilistic topic modelling like LDA and its extensions. Here topics are seen as discrete values and distribution over words from the corpus. An example of single-membership models is the Top2Vec algorithm for topic modelling and semantic search where the topic is thought of as a continuous representation.

Lastly, the very relevant for Top2Vec algorithm techniques like some dimensionality reduction methods and hierarchical density-based cluster analysis, namely HDBSCAN have been discussed. UMAP and t-SNE are non-linear techniques using for dimensionality reduction. While the first one is good at maintaining both the local and the global structure of the data and is scalable to larger datasets, the t-SNE method demonstrates more sensitivity to the local structure, which makes it suitable for data visualization. 

When dealing with unsupervised clustering algorithms, the validation procedure assumes greater importance. All clustering validity methods depending on that if they estimate the quality of clustering using the external knowledge (e.g. true labels) or base exclusively on the information intrinsic to the data itself are divided into external and internal measures. The arbitrary shaped clusters obtained with any density-based clustering algorithm can be evaluated with such external index like the adjusted Rand Score and with DBCV index as an internal one.

\chapter{Dataset}
\label{chap:dataset}
This chapter aims to provide a representation of the underlying data used to extract the topic clusters with Top2Vec topic modelling algorithm and different embedding models\footnote{More details about embeddings model the Top2Vec algorithm was tested with please see in the next Chapter (Chapter \ref{chap:exp}).}. In order to evaluate the clustering results in a supervised way, a toy corpus has been created. For the unsupervised training, a much larger dataset was used. 

The overview of the whole dataset is presented in Section \ref{sec:corp_exploration} of this chapter. The process of preparation of the dataset for cluster evaluation in the supervised and unsupervised mode is described in Section \ref{sec:exp_setup}.

\section{Corpus description}
\label{sec:corp_exploration}

The underlying dataset was provided by the Ippen Digital GmbH \& Co. KG and consists of the German news articles from various regional newspaper publishers like Merkur, TZ, Frankfurter Rundschau, etc. The articles covered a broad range of topics from local news, business and politics to sport news, TV and articles on the automotive industry.

The data were extracted from the in-house database for the period of January to April 2021. Each document from this database apart from the content contains also metadata such as a document ID, length (the number of characters in the article), "client" (what newspaper publisher the article comes from), "category", "headline", seo headline and some other information. The exemplary document is demonstrated in the Listing \ref{listing:json-example}.

To create a dataset, all documents within the mentioned period were filtered by the following categories: "Auto", "Games", "Politik" (Politics), "Fußball" (Football), "Sport", "Kultur" (Culture), "Serien" (Series), "Film, TV \& Serien", "Serien, TV \& Kino" (Series, TV \& Cinema). The last three categories were considered as belonging to the one topic since they have similar content, but differ only in their name depends on the newspaper they belong to. For the clustering evaluation after training, this category was unified as a "Serien" one. The entire dataset consists of the 17.532 documents, that was extracted with all available metadata and saved as several JSON files.

Before training the content of the documents was cleaned while the lines that have nothing to do with the article itself like HTML hyperlinks or some metadata (e.g., © dpa-infocom) were removed.


\begin{listing}[ht!]
\begin{minted}[frame=single,
            style=tango,
            showtabs=false,     
            framesep=3mm,
            escapeinside={\%*}{*)},        
            linenos=true,
            breaklines=true,
            xleftmargin=21pt,
            tabsize=4]{json}
{
    "doc_id": 90161825, 
    "length": 998, 
    "client": "Merkur.de", 
    "clientid": 268, 
    "category": "Politik", 
    "headline": "Impfbereitschaft in Deutschland nimmt zu", 
    "seo_headline": "Impfbereitschaft in Deutschland nimmt zu", 
    "publishing_date": "2021-01-07 19:01:18", 
    "content": " Berlin (dpa) - Die Bereitschaft zur Impfung gegen das Coronavirus hat laut einer Umfrage in Deutschland zugenommen. In einer Befragung von infratest dimap für den ARD-Deutschlandtrend von Anfang der Woche gaben 54 Prozent an, sich auf jeden Fall gegen das Coronavirus impfen lassen zu wollen. Das waren 17 Prozentpunkte mehr als im November 2020. Weitere 21 Prozent sagten, sie wollten sich wahrscheinlich impfen lassen (minus 13 Prozentpunkte). Die Impfbereitschaft sei in allen Altersgruppen gewachsen, insbesondere aber bei den Menschen unter 65 Jahren. Nach wie vor zeigten sich jüngere Menschen gegenüber einer Corona-Impfung weniger offen als ältere. Rund 36 Prozent der Befragten bezeichneten das Tempo der begonnenen Corona-Impfungen als angemessen, mehr als die Hälfte (52 Prozent) empfinden es als zu langsam. 70 Prozent erklärten, es sei richtig gewesen, dass die EU-Länder die Impfstoffe gemeinsam bestellt haben. 26 Prozent halten das für falsch. © dpa-infocom, dpa:210107-99-936480/2", 
    "image": 25393708, 
    "modification_date": "2021-01-07 19:06:30"
}

\end{minted}
\caption{The example of the document from the in-house database.} 
\label{listing:json-example}
\end{listing}
 
\section{Dataset preparation for cluster evaluation in supervised way}
\label{sec:exp_setup}

Because depends on the embedding and UMAP and HDBSCAN parameters, the Top2Vec topic modelling algorithm leads to different results, it is important to run it on small subparts of the dataset. This makes it possible to run the algorithm in a semi-supervised way and to measure the quality of the resulting clusters not only with internal DBCV index \parencite{Moulavi14}, but also with the external one, namely adjusted Rand index \parencite{Hubert85}. 

For this purpose, a toy corpus was created, while from the whole dataset only articles for March 2021 was chosen and then filtered by the following categories: "Auto", "Games", "Fußball", "Serien" (and its name variations). These categories were selected in order to have articles with clearly separated content, whereas some words between at least three categories ("Games", "Fußball", "Serien") can be shared. That has made the algorithm's behaviour more transparent.

The final toy corpus consists of 1.640 documents with the following document distribution among four categories:

\begin{table}[ht]
    \centering
    \begin{tabular}{ |c|c| } 
        \hline
        \textbf{Category} & \textbf{Number of documents} \\
        \hline
        Auto & 240 \\
        \hline
        Games & 254 \\
        \hline
        Fußball & 840 \\
        \hline
        Serien & 306 \\
        \hline
    \end{tabular}
    \caption{Document distribution within each of the four selected categories in the toy corpus.}
    \label{tab:doc_dist}
\end{table}

Considering that the Top2Vec algorithm is prone to find a lot of subtopics, it is expected to have more than four clusters after the training. Therefore, in order to measure the quality of these clusters, all articles were manual annotated. 

The annotation process was realized in four steps. Firstly, underlying subtopics per each category were identified. Secondly, each document was assigned a label that corresponds to the subtopic of the document. Thirdly, a dictionary with the mapping of the corresponding subtopic (label) to its relevant keywords was created. And finally, since for the computation of the validity score the adjusted Rand index requires not labels, but true and predicted topic IDs all manual created labels a corresponding ID was assigned based on the matching between topic and label keywords (the same ID as that of the topic in case of match or a new ID).

Lastly, after the process of UMAP and HDBSCAN parameter selection with each available embedding was done and the set of parameters with that Top2Vec algorithm output the best clusters on toy corpus was found, the algorithm with these parameters was applied to the entire dataset.

\chapter{Experiments}
\label{chap:exp}

In this chapter the experiments with relevant methods explained in Chapter \ref{chapter:theory} and implementation aspects will be presented. The baseline model is addressed in Section \ref{sec:lda_baseline}. Experiments itself are explained in Section \ref{sec:exp}. It includes the tests with different embedding models (both suggested by Top2Vec algorithm and additionally integrated into that to obtain German embedding) (Section \ref{sec:exp_emb}) and hyperparameter selection process to get a better clustering (Section \ref{sec:hypeparam}).

\section{Test with LDA as baseline topic model}
\label{sec:lda_baseline}

Since topic modelling belongs to an unsupervised learning, it is a hard to find an optimal heuristic for it. Therefore to understand the context of topic modelling with the underling data and possible challenges that could be helpful to find better more complex solution a simple baseline model is needed. As a such baseline model Latent Dirichlet Allocation (LDA) considered in Section \ref{sec:lda} of Chapter \ref{chapter:theory} was chosen, since it is the most common type of topic modelling.

The LDA algorithm was run on the small dataset with 200 iterations\footnote{The number of iterations was determined based on the log likelihood parameter.}. Since documents in this dataset belong to four different clearly separated categories, it was assumed there are four underlying general topics in the data and consequently the total number of topics to be obtained (parameter \(K\)) was set to 4. For the reason that the resulting cluster shall be quite homogeneous, the small alpha (0.5) was chosen. Before training a necessary preprocessing like tokenization, lemmatization and stemming, as well as removing stop word was done.

\renewcommand{\arraystretch}{2}
\begin{table}[ht]
    \centering
    \begin{tabularx}{\textwidth}{ |c|c|X| } 
        \hline
        \textbf{Topic Id} & \textbf{Freq} & \textbf{Topic Keywords} \\
        \hline
        1 & 0.34720 &  löw, sommer, bundestrainer, trainer, spieler, dfb, deutschland, fc bayern, fußball, münchen \\
        \hline
        2 & 0.25652 & werder bremen, saison, tor, mannschaft, sieg, sv werder bremen, ball, trainer, partie, werder \\
        \hline
        3 & 0.24969 & auto, netflix, staffel, adac, serie, prozent, autofahrer, deutschland, fahrzeug, sony \\
        \hline
        4 & 0.14658 &  love island, staffel, adriano, folge, bianca, script, rtl, emilia, kandidaten, liebe \\
        \hline
    \end{tabularx}
    \caption{LDA Topics with topic score and topic keywords.}
    \label{tab:lda_topic_keywords}
\end{table}
\renewcommand{\arraystretch}{1}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{LDA_1.png}
\caption{pyLDAvis visualization of identified topics with the word distribution for the \(1^{st}\) topic.}
\label{fig:lda_topics}
\end{figure}


As it could be seen from Table \ref{tab:lda_topic_keywords} representing LDA topics and their keywords, the two first topics are semantically very similar. Both of them cover the football topic and are actually the subtopics of it. Furthermore, like Figure \ref{fig:lda_topics} shows both subtopics are overlapped. Speaking about the third and the fourth topics, although they are well separated like it is demonstrated in Figure \ref{fig:lda_topics}, they are not defined very well. Based on the keywords of the third topic, it is not clear whether it is a Series or a Football cluster. The fourth topic seems to represent a Series cluster, while the Games theme is not presented at all.

However, at this place, it is worth mentioning that the articles' distribution in the dataset (both small one and the entire data set) is rather unbalanced, since like it was already illustrated in Table \ref{tab:doc_dist}, the category Football has the highest number of articles comparing with the others three categories. Nevertheless, because this is the real data, and they are usually unbalanced, it could be seen as a challenge, with that a topic modelling algorithm has to deal.

\section{Experiments with Top2Vec and different embedding techniques}
\label{sec:exp}

In this section all experiments with Top2Vec topic modelling algorithm that was conducted on German data are presented. Firstly, the experiments on small dataset was made. Here for the clustering evaluation, two validity indexes described in Chapter \ref{chapter:theory} (Subsection \ref{sec:validation}) were used: Density-Based Clustering Validation index (DBCV) \parencite{Moulavi14}, which is an internal metric and the external one, namely adjusted Rand Index \parencite{Hubert85}. Then Top2Vec algorithm with the parameter setup achieved one of the best results per each embedding model was run on the whole dataset. In this case the resulting clusters were evaluated only with internal DBCV index, since for these data no ground truth labels are available.

Please note, that the clustering validity index alone is not enough to access the validity of clusters and an additional instrument like visualization is needed \parencite{Halkidi01b}. So to verify how homogeneous the resulting clusters are and to check the assignment of topics to documents after each training an HTML file was created. This file contains for all trained data both the original information that comes from the in-house database and model's output. The original information includes such metadata like document ID, client, category, headline and content. The output of the trained model consist of the assigned to each document topic ID and topic keywords. The Listing \ref{listing:data-example} demonstrates the same document as the Listing \ref{listing:json-example} saved after training with some original metadata and information from the Top2Vec model.

\begin{listing}[ht]
\begin{minted}[frame=single,
            style=borland,
            showtabs=false,     
            framesep=3mm,
            escapeinside={\%*}{*)},        
            linenos=true,
            breaklines=true,
            xleftmargin=21pt,
            tabsize=4]{html}
<tr>
  <td>90161825</td>
  <td>tz</td>
  <td>Politik</td>
  <td>Impfbereitschaft in Deutschland nimmt zu</td>
  <td>Berlin (dpa) - Die Bereitschaft zur Impfung gegen das Coronavirus hat laut einer Umfrage in Deutschland zugenommen. In einer Befragung von infratest dimap für den ARD-Deutschlandtrend von Anfang der Woche gaben 54 Prozent an, sich auf jeden Fall gegen das Coronavirus impfen lassen zu wollen. Das waren 17 Prozentpunkte mehr als im November 2020. Weitere 21 Prozent sagten, sie wollten sich wahrscheinlich impfen lassen (minus 13 Prozentpunkte). Die Impfbereitschaft sei in allen Altersgruppen gewachsen, insbesondere aber bei den Menschen unter 65 Jahren. Nach wie vor zeigten sich jüngere Menschen gegenüber einer Corona-Impfung weniger offen als ältere. Rund 36 Prozent der Befragten bezeichneten das Tempo der begonnenen Corona-Impfungen als angemessen, mehr als die Hälfte (52 Prozent) empfinden es als zu langsam. 70 Prozent erklärten, es sei richtig gewesen, dass die EU-Länder die Impfstoffe gemeinsam bestellt haben. 26 Prozent halten das für falsch.</td>
  <td>53</td>
  <td>[forsa, prozentpunkte, umfrage, prozentpunkt, insa, befragten, prozentpunkten, erhebung, prozent, umfragen, umfragewerte, bundestagswahl, infratest, dimap, bundesburger, grunen, auftrag, unionsparteien, unverandert, kanzlerkandidat, befragung, minus, wahlern, befragt, yougov, fdp, wahler, kanzlerschaft, wahlen, kame, annalena, baerbock, habeck, landtagswahlen, unions, ampelkoalition, cdu, laschet, zweitstarkste, ergab, kanzler, zustimmung, zuwachs, spd, armin, gleichauf, union, vergleich, regierende, wahlberechtigte]</td>
</tr>
\end{minted}
\caption{The same document with some metadata and an information from the trained model (topic ID: 53 and topic keywords).} 
\label{listing:data-example}
\end{listing}

Then using all this information clusters were visualized\footnote{For the clusters visualization the t-SNE dimensionality reduction algorithma and interactive data visualization Python library \href{https://bokeh.org/}{Bokeh} was used.} such that each data point that denotes a document got an annotation label that contains the ID of the corresponding document, its headline as well as assigned topic ID and topic keywords. Figure \ref{fig:cluster_vis} shows the example of the cluster visualization for one of the trained model. The topic clusters itself are plotted by the combination of different colours and shapes of the points, the IDs of each cluster are represented on the right side. The label annotation for every point is interactive. 

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{vis_example.png}
\caption{The example of cluster visualization with t-SNE.}
\label{fig:cluster_vis}
\end{figure}


\subsection{Experiments with embedding models}
\label{sec:exp_emb}

Apart from doc2vec....


The multilingual Universal Sentence Encoder embedding model was trained on different texts from 16 languages like English, German, French, Arabic, Chinese, etc. Its training data consists of question-answering pairs from online forums and QA websites\footnote{Reddit, StackOverflow, and YahooAnswers.}, translation pairs, and the Stanford Natural Language Inference (SNLI) corpus \parencite{Cer18c}. Since the SNLI corpus contains only English sentences, to obtain them in other 15 languages, the sentences were translated with Google's translation system \parencite{Cer18c}. The resulting training corpus amounted to 8 million sample of sentences.

The next three embedding models are based on Sentence-Bert architecture \parencite{Reimers19}. For the training all of them the parallel data from nine different datasets\footnote{Such datasets include GlobalVoices corpus with news stories, News-Commentary dataset with political and economic commentaries, WikiMatrix with parallel sentences from Wikipedia, and some other translated texts collections.} was used. The first SBERT based model is \textquote{Distiluse-base-multilingual-cased} pre-trained model, which is multilingual distilled version of multilingual Universal Sentence Encoder and supports 15 languages. 


Another bilingual model is Cross English \& German RoBERTa for Sentence Embeddings


This model is based on XLM-RoBERTa (XML-R) model, that was trained on more than 50 Million paraphrases pairs \parencite{Reimers20}.
 large scale paraphrase dataset for 50+ languages. 







\subsection{UMAP and HDBSCAN hyperparameter optimization}
\label{sec:hypeparam}
blabal


\chapter{Results and evaluation}

blablabla

\section{Top2Vec with doc2vec embedding}
\label{sec:doc2vec_results}
blabal

\section{Top2Vec with Multilingual universal sentence encoder}
\label{sec:muse_results}

blablabla

\section{Top2Vec with Sentence Transformers}
\label{sec:senttrans_results}
blablabla


\subsection{"Distiluse-base-multilingual-cased"}
\label{sec:muse_result}
blabal


\subsection{German RoBERTa for Sentence Embeddings V2}
\label{sec:roberta_de_result}
blabal

\subsection{Cross English \& German RoBERTa for Sentence Embeddings}
\label{sec:roberta_de_en_result}
blabal

\section{Discussion}
\label{sec:discussion}
blablabla


\chapter{Conclusion and Future Work}
blablabla

\section{Conclusion}
blablabla

\section{Future Work}
blablabla


\newpage



% Imports the bibliography file "sample.bib" 
%Includes "Bibliography" in the table of contents
\printbibliography[heading=bibintoc]


\newpage

% Abbildungsverzeichnis (kann auch nach dem Inhaltsverzeichnis kommen)
\listoffigures
\newpage

% Tabellenverzeichnis (kann auch nach dem Inhaltsverzeichnis kommen)
\listoftables
\newpage

% \lstlistoflistings
% % \addcontentsline{toc}{section}{Listings}
% \newpage


\addchap{Inhalt der beigelegten CD}

\end{document}

