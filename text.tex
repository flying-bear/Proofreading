\documentclass[fontsize=12pt,a4paper,twoside,openany]{scrbook}
\usepackage{clba}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[german,english]{babel}

% to link the references
\usepackage{hyperref}
\hypersetup{colorlinks=true,
            linkcolor=blue,
            filecolor=magenta,      
            urlcolor=cyan,
            citecolor=[rgb]{0,0.4,0},
            pdfpagemode=FullScreen,}

% listing json file
\usepackage{listings}
\usepackage{minted}
\usepackage{xcolor}

% two figures side by side
% \usepackage{subcaption}

% break lines in table
% \usepackage{makecell}
\usepackage{tabularx}
% \newcolumntype{Y}{>{\centering\arraybackslash}X}

% to use list of abbreviations
% \usepackage[acronym]{glossaries}

% not to be hyphenated
\hyphenation{HDBSCAN}

% add blank line between paragraphs
\setlength{\parskip}{1em}

% Import the biblatex package and sets a bibliography style
\usepackage[backend=biber, style=authoryear,parencitestyle=authoryear,bibstyle=numeric]{biblatex}
\addbibresource{bibfile.bib} %Imports bibliography file

% Per Kapitel Nummerierung von Graphiken und Tabellen
\counterwithin{figure}{chapter}
\counterwithin{table}{chapter}

% Hier die eigenen Daten eintragen
\global\fach{Computational Linguistics }
\global\arbeit{Master's Thesis}
\global\titel{The Validation of Topic Clustering based on Top2Vec Topic Modeling Method}
\global\author{Nataliia Sherstneva}
\global\betreuer{M.Sc. Lütfi Kerem Şenel}
\global\pruefer{Prof. Dr. Hinrich Schütze}
\global\universitaet{Ludwig-Maximilians-Universität München}
\global\fakultaet{Faculty of Languages and Literatures}
\global\department{Department 2}

\global\abgabetermin{26 July 2021}
\global\bearbeitungszeit{12 March - 04 July 2021}
\global\ort{Munich}

\begin{document}

% Deckblatt
\deckblatt
\pagestyle{scrheadings}
\pagenumbering{gobble}

% Erklärung fürs Prüfungsamt
\erklaerung

% Zusammenfassung
\addchap{Abstract}
\pagenumbering{Roman}
\thispagestyle{scrplain}
\noindent
Dieses Dokument dient als Muster für eine Ausarbeitung einer
Bachelorarbeit am CIS und wird in deutscher oder englischer Sprache
erstellt (hier max. 250 Wörter)

% Acknowledgements
\addchap{Acknowledgements}
\thispagestyle{scrplain}
I would like to thank...

% Inhaltsverzeichnis
\tableofcontents

\chapter{Introduction}
In the past several decades...

% Text mit arabischer Nummerierung
\pagenumbering{arabic}

\section{Motivation}
Here comes Motivation...

\section{Contribution}

In particular no study, to our knowledge (to the best of my knowledge), has considered ....


no previous research has investigated

There is no previous research using ... approach.

\section{Outline}
The rest of the thesis is organized as follows.


\chapter{Related Work and Theoretical Framework}
\label{chapter:theory}
In recent years, computer science has greatly advanced in deep learning architectures which enable the use of new approaches and techniques for data and text analysis \parencite{Minaee20} and which combine such methods to achieve better performance. Since the main topic of the present paper, Top2Vec algorithm, utilises several such techniques for searching a topic cluster in semantic space, this chapter aims to give an overview of all the relevant methods. 

The rest of the chapter is organized as follows. In Section \ref{sec:A} such state-of-the-art deep learning embedding techniques as Paragraph Vector or \verb|doc2vec| (Section \ref{sec:doc2vec}) and Transformer-based sentence embedding (Section \ref{sec:sbert}) are presented. An overview of various topic modelling approaches is provided in section \ref{sec:B}. Section \ref{sec:C} is dedicated to density based clustering, specifically HDBSCAN algorithm (section \ref{sec:HDBSCAN}) and clustering validation methods (\ref{sec:validation}).  

\section{Embedding techniques in text analysis}
\label{sec:A}

The introduction of embeddings was an important step in machine learning that began a new era in natural language processing. Before embeddings, the numeric representation of texts was acquired using simple approaches like one-hot encoding and the Bag-of-Words (BOW) representation of documents in the form of the count vectorization. These techniques, however, have some weaknesses. One of the most crucial drawbacks for text representation is the disregard for the word semantics. It leads to the fact that similar words are treated as completely unrelated (e.g., \emph{jump} and \emph{leap}). The introduction of distributed word representation learning methods rendered it possible to capture syntactic and semantic relationships between the words \parencite{Le14}.

The first method aimed at learning dense vector representation of words with neural networks, known as the \verb|word2vec| algorithm, was introduced by Tomas Mikolov \parencite{TMikolov13} (see also \parencite{Mikolov13}). It is based on the so-called \textquote{distributional hypothesis} \parencite{Harris54}. The main idea of the hypothesis was expressed by the English linguist John Rupert Firth, who said that a word is characterized by the company it keeps \parencite{Firth57}.

Having a numerical representation for separate words in a document naturally raises the question of whether it is possible to create continuous distributed vector representations for larger pieces of texts like a sentence, paragraph, or even an entire document, regardless of its length. So, in 2014 Mikolov and Le proposed Paragraph Vector Framework also known as \verb|doc2vec| which was an extension of the \verb|word2vec| algorithm \parencite[see][]{Le14}.

Another state-of-the-art approach to sentence embeddings uses Transformers. Since Transformers apart from word embeddings also use positional information about the input (positional embedding), they are able to take into account the contextual information and therefore to better understand the context and semantics of the input sequence. There are a lot of methods based on Transformer architecture which attempts to obtain embedding for a large span of the text, e.g. sentences. The most common of the Transformer-based methods are: Skip-Thought Vectors, which provide generic sentence representations \parencite{Kiros15}, and their variation, FastSent \parencite{Hill16}, and several supervised sentence embedding techniques, namely, InferSent \parencite{Conneau17}, proposed by Facebook AI Research in 2018, Universal Sentence Encoder, and Sentence BERT. 

Due to time restrictions, not every embedding model could be tested in the present study. Thus, in the next sections, only the most relevant methods of getting numeric texts representation are considered. Firstly, \verb|word2vec| embedding model is covered briefly and then \verb|doc2vec| algorithm, which is based on it, is discussed in more detail (Section \ref{sec:doc2vec}). Among Transformer-based methods, Universal Sentence Encoder (Section \ref{sec:muse}) and Sentence-BERT (Section \ref{sec:sbert}) are considered. Both Transformer-based models are used in Top2Vec topic modelling and, according to the previous experiments, yield the best performance on the downstream tasks \parencite{Wang20}.

\subsection{Paragraph Vectors Framework (doc2vec)}
\label{sec:doc2vec}

\verb|word2vec| model uses a simple feed-forward neural network to calculate a vector for each word by concatenating or averaging it with other word vectors in a context (\textquote{window}). As a result, each word is mapped to a continuous low-dimensional vector. The semantic similarity is captured by the distance in the resulting vector space. In other words, the word-vectors of similar words are close to each other and the vectors of dissimilar ones are distant from each other.

The word embedding model for learning word vectors consists of two frameworks: the continuous Bag-of-Words (CBOW) model and the continuous Skip-gram model. Their architectures are depicted in Figure \ref{fig:cbow_skip_gr}. The first algorithm, CBOW, tries to predict a target word based on its surrounding context words. The goal of the second, continuous Skip-gram model, is to predict the context words based on a target word.

\begin{figure}[h]
\centering
\includegraphics[width=0.90\textwidth]{cbow_skip_gr.png}
\caption{The CBOW and Skip-gram architectures from \parencite{TMikolov13}}
\label{fig:cbow_skip_gr}
\end{figure}

The architecture of Paragraph Vector Framework (\verb|doc2vec|) is based on the previous method for learning word vectors, the only difference being that the prediction of the next word uses an additional document-unique feature vector alongside the word-vectors. This way, each paragraph (or a larger text) is mapped to this additional vector represented by a column in matrix \emph{D}, while words are still represented by a column in matrix \emph{W}, just like in the \verb|word2vec| algorithm. 

There are two frameworks of the \verb|doc2vec| Model: the Distributed Memory Model of Paragraph Vector (PV-DM) and Distributed Bag of Words version of Paragraph Vector (PV-DBOW).

In the Distributed Memory Model, like in \verb|word2vec| CBOW, every paragraph or document is mapped to a unique feature vector as well as every word. For the prediction task, the paragraph vector is concatenated or averaged with the context word vectors (see Figure \ref{fig:pv-dm}). Here, the paragraph vector acts as a memory of the paragraph or document topic, since it contains the information missing from the current context. After training, this vector can be used as a representation for the paragraph or document in downstream NLP tasks.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.70\textwidth]{pv-dm}
\caption{The architecture of PV-DM Model for an input sentence}
\label{fig:pv-dm}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.70\textwidth]{pv-dbow}
\caption{A distributed Bag of Words model of Paragraph vectors}
\label{fig:pv-dbow}
\end{figure}

The other Paragraph Vectors framework is similar to the \verb|word2vec| Skip-gram model. The paragraph vector is here simplified, and no local context is used. The goal of this model is to predict the context words, randomly sampled from the paragraph vector \parencite{Le14}, as shown in Figure \ref{fig:pv-dbow}. Despite the simplicity of this model, it has a better performance of the two for the topic modelling task \parencite{Angelov20}.

The Paragraph Vectors Model has several important advantages. First of all, Paragraph Vectors capture the semantics of the words and even document semantics in dense vectors, so that the similar words (or documents) get the similar vectors and are located closer to each other in the vector space. Secondly, the Paragraph Vectors Model can be applied to the text of any length and does not require a task-specific tuning. Thirdly, the word context is taken into account \parencite{Dai15}. And finally, no labelled data are needed for the training process. 

All of that makes Paragraph Vectors Framework very promising for various NLP tasks, such as similarity search, text classification, and topic modelling. Experiments show that on the Semantic Textual Similarity Task (STS) \verb|doc2vec| model usually outperforms not only Bag-of-Words model but also the \verb|word2vec| one \parencite[see][]{Lau16, Le14}.

\subsection{Universal Sentence Encoder}
\label{sec:muse}

The Universal Sentence Encoder was developed by Google and presented in \parencite{Cer18a} and \parencite{Cer18b}. The concept of the Universal Sentence Encoder (USE) is actually an umbrella term for a family of TensorFlow sentence encoding models. Currently, it includes 11 models based on multitask training, and hence their sentence embeddings (or high-dimensional vectors) can be used for such tasks as text classification, clustering,  fine-grained question-answer retrieval, semantic similarity search, etc.\footnote{Collection of universal sentence encoders. \url{https://tfhub.dev/google/collections/universal-sentence-encoder/1}}

The USE method consists of two encoding models for sentence representation learning: one of them is based on the Transformer encoder architecture, while the other one uses the Deep Averaging Network (DAN) model \parencite{Cer18a} (see Figure \ref{fig:use_architecture}).

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]{use_architecture.png}
\caption{The two models of the Universal Sentence Encoder: (a) Transformer Encoder and (b) DAN}
\label{fig:use_architecture}
\end{figure}

The architecture of the first model consists only of the Encoder part of the original Transformer architecture, presented in \parencite{Vaswani17}. Not using the Decoder part allows to significantly reduce the training time, while keeping the performance during transfer learning. The USE Transformer architecture consists of six stacked Transformer encoder layers, where each of them has a self-attention layer followed by a feed-forward network. Generating a word embedding with self-attention takes the word order and the surrounding context into account \parencite{Cer18a}. Then the context-aware embedding representation of the sentence words is averaged together to get an output, which is a sentence level 512-dimensional embedding vector. 

The advantage of this model is that it has good accuracy on downstream tasks. However, since the self-attention has \(O(n^2)\) time complexity, the time of computation scales significantly if the length of the sentence increase. \footnote{Therefore, the sequence length of the input has a fixed size of 512 tokens.}

The second variant of USE is based on the Deep Averaging Network (DAN) \parencite{Iyyer15}. In this model, the embedding of uni-grams and bi-grams are computed and averaged into a single embedding. The averaged embedding is then fed into a deep feed-forward neural network, yielding a final sentence embedding, again a 512-dimensional vector.

In contrast to the Transformer variant, the DAN encoder has more efficient inference time, since it does not have any self-attention layers and uses only feed-forward operations, the time complexity of which is linear and equal to the length of the input sentence, that does not have fixed number of tokens. However, this USE model has lower accuracy than the Transformer USE variant. 

Most pre-trained sentence embedding models are language-specific. To be able to process an input of a particular language and to produce an embedding for it, the model should be trained for this language. The crucial difference of a multilingual USE is that it is trained simultaneously on multiple tasks across languages. The deep learning technique that enables multilingual sentence embeddings is called \textquote{Multi-task Dual-Encoder Model} \parencite{Chidambaram19}. This allows to compare the sentences from different languages, since the embeddings of texts in different languages with similar meaning are closer to each other than the embeddings of dissimilar texts. 
Testing the multilingual USE as a pre-trained embedding model for the Top2Vec algorithm was performed as a part of this thesis (see Section \ref{sec:muse_results}).

\subsection{Sentence-BERT}
\label{sec:sbert}

Until recently, BERT was one of the common methods to get numerical sentence representation \parencite{Devlin19}. Different layers of this pre-trained Transformer network learn different linguistic properties, and thus it is possible to get a sentence representation by merging information obtained from these different layers and either averaging the output layer across tokens or using an output of the first [CLS] token \parencite{Wang20}. However, the quality of embedding resulting from this method was not very high and due to the very large model size the complexity of finding the most similar sentence pair was also very high (ab. 65 hours) \parencite{Reimers19}. These limitations make BERT not suitable for similar search task on large corpora.

New state-of-the-art algorithm in sentence embedding is called Sentence-BERT \parencite{Reimers19}. As the name implies, it is also based on BERT and Transformer architecture. Like BERT, Sentence-BERT uses the attention mechanism, which allows the model to create an embedding by focusing only on the most important parts of the text input. Yet, Sentence-BERT modifies the architecture of BERT (or its extensions like RoBERTa) by adding a pooling operation to its output \parencite{Reimers19}.

The crucial point of Sentence-BERT architecture is the usage of the so-called siamese and triplet network. As shown in Figure \ref{fig:S-BERT-arch}, Sentence-BERT takes two sentences from the pre-trained BERT model and then uses the polling layer to generate the embedding for each sentence. Then they are concatenated and a softmax function is applied. For the classification task, the embeddings of two sentences \emph{u} and \emph{v} is used as an input to compute the cosine similarity score.


\begin{figure}[h]
\centering
\includegraphics[width=0.90\textwidth]{S-BERT-arch.png}
\caption{SBERT architecture with classification objective function for fine-tuning (left) and at inference with regression objective function (right) \parencite[from][]{Reimers19}}
\label{fig:S-BERT-arch}
\end{figure}

These modifications make Sentence-Bert suitable for such tasks as clustering or semantic similarity search, for which BERT is unsuitable. The evaluation of this state-of-the-art algorithm for the common STS task shows that it not only has higher computational efficiency than other state-of-the-art sentence embedding methods like InferSent of Universal Sentence Encoder, but also outperforms them.

To obtain a multilingual and cross-linguistic sentence embedding, SBERT model was extended to other languages through the use of the multilingual knowledge distillation method \parencite{Reimers20}. Within this approach, one model is treated as a student model and another one as a teacher model. Based on parallel sentence data, from the teacher model the student model adopts a multilingual semantic embedding space for the original source language, which is usually English, and then transfers it to other languages \parencite{Reimers20}. 

\section{Topic modelling}
\label{sec:B}

A numeric representation of sentences or larger text fragments is a prerequisite to finding their semantic similarity. However, when working with a large corpus, it is often also desirable to derive specific topics that occur in it or topic clusters. This is one of the common and well-developed tasks in modern computational linguistics. Thus, this section of the paper is dedicated to topic modelling, which is one of the text analysis methods that enable automatic extraction of topics present in a corpus.

Topic modelling belongs to the unsupervised machine learning techniques, which means that discovering semantic structures in a text and identifying the topics present in a corpus does not require any prior knowledge about the data and true classification of the documents in the corpus. Topic modelling is not the only technique used for identifying topics within the texts via analysing word groups. Some other methods, like latent semantic analysis (LSA) or cluster analysis are also aimed at it \parencite[see][]{Foltz96, Landauer2007}. In both methods, the document-term matrix (DTM) or term-document matrix (TDM) is used as the input for the analysis. LSA, f.e. tries to detect latent meaning in the documents through dimension reduction of TDM representation of corpora \parencite{Anandarajan18}. In contrast to LSA, the aim of cluster analysis is not to reveal a \emph{hidden} semantic structure within the document, but to discover, based on distance and similarity measurements, a set of underlying topics in the whole document collection by groping the similar items together \parencite{Anandarajan18}. Topic modelling concentrates on the underlying themes that appear in a document collection and determines the probability that each document is associated with a given topic. 

There are two types of topic models: mixed-membership and single-membership models. In the first type of modelling, each document belongs to several topics with a different probability distribution, in the second one, each document can be assigned only one topic. Latent Dirichlet Allocation (LDA) model is an example of the mixed-membership topic modelling, while the Top2Vec topic modelling algorithm represents the single-membership models. In the next two subsections, these techniques are covered in more detail.

\subsection{Probabilistic topic modelling}
\label{sec:lda}

In the probabilistic topic modelling, the data are viewed as arising from a generative process \parencite{Blei12}. Each document is understood as a combination of topics and every topic is seen as a probability distribution of words within it. 

The most common form of probabilistic topic modelling is latent Dirichlet allocation introduced in \parencite{Blei03}. LDA is based on the bag-of-words assumption, which means the word order is disregarded when building a topic model. Thus, each word that appears in the corpus is randomly assigned to one of the topics with certain probability scores as illustrated in Figure \ref{fig:lda_intuition}. Assuming Dirichlet prior distribution for both document-topic and topic-word distributions, LDA treats any topic as a distribution over words from the corpus and each document as a collection of corpus-wide topics \parencite{Blei03}.

\begin{figure}[h]
\centering
\includegraphics[width=0.80\textwidth]{lda_intuition.png}
\caption{The intuition behind the LDA algorithm. From \parencite{Blei12}}
\label{fig:lda_intuition}
\end{figure}

LDA topic model has several input parameters:
\begin{itemize}
  \item \(K\) is the total number of topics that should be extracted from all the documents in the corpus;
  \item \(D\) is the total number of documents;
  \item \(N\) denotes the total number of words in a document \emph{d}, where W\(_{d,n}\) is an observed n\(_{th}\) word in that document.
  \item \(\alpha\) is a Dirichlet parameter which denotes the density of per-document topic distribution,
  \item \(\beta\) is another Dirichlet parameter which is responsible for per-topic word distribution,
  \item \(\eta\) is a Dirichlet topic hyperparameter.
\end{itemize}

All these parameters are represented in Figure \ref{fig:lda} that is a plate representation of the generative process assumed in LDA. The biggest outer plate should be considered as a corpus of documents, while the inner plate stands for the repeated choice of topics \(Z\) and words \(W\) within a document \(N\) \parencite[see also][p.~997]{Blei12}. Each document \emph{d} is seen as a combination of topics with topic proportions \(\theta_{d}\) coming from Dirichlet(\(\alpha\)). Each topic \(k\) has a distribution over words \(\beta_{k}\) from Dirichlet(\(\eta\)). In each document, each word W\(_{d,n}\) is drawn from a topic \(Z_{d,n}\) with a topic assignment \(\beta_{k}\). 

\begin{figure}[h]
\centering
\includegraphics[width=0.70\textwidth]{lda.png}
\caption{Graphical representation of the random variables (nodes) in the LDA model with their dependencies (edges).}
\label{fig:lda}
\end{figure}

Informally, in LDA topic modelling, every term in a document can come from a different topic or can be seen as belonging to different topics with a different probability score. Every document can be assigned to several topics also with a different probability score.

Although LDA is one of the most widely used topic modelling algorithms, it has some limitations. First, as the model is based on the bag-of-words representation of documents, it does not preserve the meaning coming from word order and thus partially ignores word semantics. To address this problem, techniques like stemming and lemmatization are used as pre-processing. However, that does not help to highlight the similarity of words with different word stems (like \emph{huge} and \emph{big}). Second, LDA assumes that the number of topics presented in the corpus is known in advance and is required by the algorithm as a hyperparameter (\(K\)). Unfortunately, the quality of extracted topics clusters strongly depends on this value, which, in most cases, is unknown and usually cannot be chosen intuitively, thus requiring a lot of time-consuming tests. 

LDA is only one example of probabilistic topic modelling. Within this paradigm, there are other approaches, such as correlated topic model (CTM) \parencite{Blei05}, dynamic topic model (DTM) \parencite{Blei06}, structural topic model (STM) \parencite{Roberts13}, etc. All of these approaches use LDA as the basis for the model with some extensions.

\subsection{Topic modelling with Top2Vec}
\label{sec:Top2Vec}

Top2Vec, proposed by Angelov in \cite*{Angelov20}, is a new unsupervised algorithm for topic modelling and semantic search. In contrast to LDA, it does not require any pre-processing like removing stop-words, stemming, or lemmatization, and can automatically detect the number of topics present in a large corpus. Moreover, the Top2Vec algorithm uses semantic embedding instead of the Bag-of-Words representation, improving the ability of the model to capture word similarity.

To identify the topic clusters in a collection of documents, Top2Vec first converts all document to high-dimensional embedding vectors. In this semantic vector space representations of similar documents are located closer to each other, forming dense areas of document vectors, while dissimilar ones are located further apart from each other creating sparse areas. Then, Top2Vec identifies dense clusters of document vectors and uses the word vectors closest to the cluster centre to characterize the topic of each cluster.

Top2Vec is based on three algorithms: DBOW, HDBSCAN, and UMAP. To obtain semantic embedding, Top2Vec utilizes Paragraph Vectors or \verb|doc2vec| model (see Subsection \ref{sec:doc2vec}). In particular, it uses Distributed Bag-of-Words Model (DBOW) to learn jointly embedded document and words vectors. Therefore, in the generated semantic vector space the distance between similar documents is smaller than between dissimilar ones. Since word and document are embedded jointly, each document (purple points in Figure \ref{fig:sem_space}) is surrounded by its most representative words (green points in the same figure).  

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]{sem_space.png}
\caption{The illustration of semantic space from \parencite{Angelov20}}
\label{fig:sem_space}
\end{figure}

This semantic space, which consists of words and document vectors, is treated in Top2Vec as a \textquote{\emph{continuous representation of topics}} \parencite[p.~3]{Angelov20}. Accordingly, the areas with a large concentration of high-dimensional documents vectors, the dense areas, contain highly similar documents. The Top2Vec algorithm supposes that the number of these dense areas is equivalent to the number of underlying topics in the corpus. 

The next step in the topic modeling process is identifying these dense areas of documents in the vector space obtained with DBOW. For that purpose, the HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) \parencite{McInnes17b, McInnes17a} algorithm is used.\footnote{In more detail the HDBSCAN algorithm is addressed in the following subsection \ref{sec:HDBSCAN}.} However, since HDBSCAN requires low-dimensional data and the output of DBOW is high-dimensional document vectors, a dimensionality reduction using UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) algorithm \parencite{McInnes18, McInnes20} is performed.

\begin{figure}[h]
\centering
\includegraphics[width=0.90\textwidth]{umap-hdbscan.png}
\caption{UMAP dimensionality reduction and density based clustering \parencite[see also][]{Angelov20}}
\label{fig:umap-hdbscan}
\end{figure}

When dense areas with jointly embedded document and word vectors are found the topics are determined by calculating the topic vectors of each cluster as an arithmetic mean of all documents vectors from the corresponding cluster. The word vectors that are closest to the resulting topic vector are considered to be the words representative of this topic. Furthermore, despite the absence of pre-processing, like the removal of stop-words, the original paper claims that topics keywords do not contain any of them. This is due to the fact that such words are so frequent, that they appear in most documents and therefore in the semantic space they are equidistant from all of them.

\begin{figure}[h]
\centering
\includegraphics[width=0.80\textwidth]{topics.png}
\caption{Examples of some topic clusters found by Top2Vec algorithm}
\label{fig:topics}
\end{figure}

Additionally, similar topics are usually placed close to each other in the resulting semantic space. For example, in Figure \ref{fig:topics} topic 14 that deals with the Astrazeneca vaccine and its side-effects and topic 72 that concerns other COVID-19 vaccines are located close to each other. The same effect can be observed on topics 29 and 57, that both concern elections. Such topics can be grouped together with hierarchical topic reduction.


\section{Density-based clustering}
\label{sec:C}

This Section covers the density-based clustering algorithm HDBSCAN applied in Top2Vec. Since analysing high-dimensional vectors is susceptible to the so-called \textquote{curse of dimensionality} \parencite{Theodoridis08}, the dimensionality reduction of vector space is required before clustering. Thus, some dimension reduction techniques are also discussed in this section.

\subsection{Dimensionality reduction}
\label{sec:umap}

In a high-dimensional space, the vectors that constitute a cluster are very sparse. This leads to high computational complexity, which makes it difficult to find the clusters \parencite{Angelov20}. Hence, an important step before applying a clustering algorithm is to perform a dimensionality reduction algorithm and to eliminate unnecessary features.

There are several dimensionality reduction techniques. Linear dimensionality reduction methods include ones like Linear Discriminant Analysis (LDA) \parencite{Tharwat17}, Singular Value Decomposition (SVD) \parencite{Klema80} or SVD-based Principal Component Analysis (PCA) \parencite{Hotelling33, Jolliffe86}. However, real data are usually not linearly separable and therefore the methods mentioned above do not have a good performance on them. Such cases require non-linear or manifold learning techniques. 

One of such non-linear methods is Uniform Manifold Approximation and Projection (UMAP) that relies on the manifold theory and topological data analysis \parencite{McInnes18, McInnes20}. UMAP is believed to have advantages over another well-known method for dimensionality reduction, t-distributed Stochastic Neighbour Embedding (t-SNE) \parencite{Maaten08}. In contrast to t-SNE, UMAP method represents both local and global structure of the data and is scalable to larger data sets. 

UMAP has several hyperparameters, namely, the number of nearest neighbours \emph{n\_neighbors} that control to what extent local and global structure is preserved, a metric to compute the distance, and the parameter to determine the number of dimensions after the dimensionality reduction.

The dimensionality reduction with UMAP algorithm is computed in two steps. First, k nearest neighbours are extracted through building a weighted k-neighbour graph \parencite{McInnes20}, then a low dimensional layout of this graph is computed.

The t-SNE algorithm due to its sensitivity to the local structure is usually used for data visualization followed by a manual investigation\footnote{Tools such as pandas Python package and interactive data visualization library \href{https://bokeh.org/}{Bokeh} can be used to achieve higher topic interpretability of the detected clusters.} of the clusters. Here, the dimensionality reduction is performed in the following way: firstly, the similarity between data points in the high-dimensional space is transformed into a probability distribution. Then the difference between the distribution in the original high-dimensional space and that in the embedded 2D or 3D space is minimized. One of the main advantages of this method is that it can maintain the local structures and therefore is good at dealing with density-based clusters, that are arbitrarily shaped. However, this method also has several hyperparameters and, depending on their values, pays more attention either to local or to global structure.

\subsection{HDBSCAN for topic clustering}
\label{sec:HDBSCAN}

HDBSCAN method or Hierarchical-DBSCAN \parencite{Campello13, Campello15} was proposed as an improved version of two algorithms: DBSCAN clustering method (or Density-Based Spatial Clustering of Applications with Noise \parencite{Ester96}) and its extension, OPTICS algorithm \parencite{Ankerst99}.

In DBSCAN, the identification of the dense regions depends on two input parameters:  \(\varepsilon\) (\emph{epsilon}) that determines the neighbourhood radius around any point \emph{x} and the minimum number of points (\(min_{pts}\)) within this neighbourhood. The point x\(_p\) is considered as a \textquote{\emph{core object}} \parencite{Campello13}, if within its \(\varepsilon\)-neighbourhood the number of points is equal or greater than \(min_{pts}\), as shown in Figure \ref{fig:core_point} . The points that are located inside the \(\varepsilon\)-neighbourhood but do not have enough points to be a core object are called \emph{border points} (like the point \emph{y} in the Figure \ref{fig:core_point}). Other points are treated as noise since they are neither core not border objects (like the green point \emph{z} in the same Figure). The main idea of DBSCAN is that the subset of all data points is considered a cluster if each point in this subset is directly or transitively density-reachable from some core point within this cluster \parencite{Ester96}.

\begin{figure}[h]
\centering
\includegraphics[width=0.70\textwidth]{core_point.png}
\caption{The classification of points in DBSCAN clustering algorithm.}
\label{fig:core_point}
\end{figure}

This method has some disadvantages. The result of the clustering depends greatly on the \emph{epsilon} parameter which is hard to choose intuitively and set correctly. It appears that this \textquote{\emph{global} density threshold} \parencite{Malzer20} prevents detecting clusters with different densities. Another problem of DBSCAN is that it is not deterministic, since if the border point is density-reachable from more than one cluster (core points) the result, that is to what cluster it is assigned, depends on the implementation and processing order of the data. 

HDBSCAN overcomes all these limitations and does not require any \(\varepsilon\)-threshold parameter. The minimum number of neighbours (\(min_{pts}\)) is the only input parameter that should be provided to the algorithm. Given this parameter, the hierarchical structure ("HDBSCAN hierarchy" \parencite{Campello13}) w.r.t. \(min_{pts}\) is built with all possible values for the \(\varepsilon\) neighbourhood radius. In order to identify noise points and to distinguish them from the objects in the dense area the authors of HDBSCAN introduced the concept of \emph{"mutual reachability distance"} \parencite[p.~163]{Campello13} between two objects \(\textbf{x}_p\) and \(\textbf{x}_q\), that is defined as follows: \[d_{mreach}(\textbf{x}_p, \textbf{x}_q) = max\{d_{core}(\textbf{x}_p), d_{core}(\textbf{x}_q), d(\textbf{x}_p, \textbf{x}_q)\}\] 
where \(d(\textbf{x}_p, \textbf{x}_q)\) is a chosen distance (Euclidean, Manhattan, Jaccard, etc.) between these two points. It makes HDBSCAN more robust to noise and allows to renders the concept of border points unnecessary. 

Using Prim's algorithm \parencite{Mamun16} the detected data subsets can be visualized in the form of a minimum spanning tree with data points as vertices connected by edges according to the mutual reachability distance. The cluster hierarchy, that is build based on the minimum spanning tree, can be represented as a dendrogram, and the construction of the hierarchy is achieved by sorting the edges in the ascending order of the distance value. Then a varying number of flat clusters with a number of points greater than or equal to \(min_{pts}\) can be obtained by setting the \(\varepsilon\)-threshold to the particular value.

One of the greatest advantages of HDBSCAN is that it is able to find arbitrarily shaped clusters with diverse density and is highly robust to noise. These characteristics are especially important in the topic modelling context since in the semantic space the document vectors are spaced with varying density \parencite{Angelov20} and the clusters to be identified are usually arbitrarily shaped. With HDBSCAN it is possible to detect the number of clusters automatically without having any intuition about the shapes and density of the clusters. The dense areas of document vector space are assigned a positive label, while the objects located in the sparse areas get a negative noise label (-1) \parencite{Angelov20}.

\section{Clustering validation techniques}
\label{sec:validation}
Clustering belongs to the unsupervised machine learning techniques, meaning there is no prior information about the underlying structure and data distribution. Hence, the validation procedure of the resulting clusters becomes important, if a rather challenging issue. All approaches to the cluster validity assessment are divided in the literature into two main classes based on external and internal criteria \parencite{Halkidi01a}. In the next two sections, the validation measures most relevant to this thesis are discussed.

\subsection{External clustering validation methods}
External validation requires pre-existing knowledge like true data labels ("ground truth"). Therefore, this validation procedure can be regarded as a supervised validation approach \parencite{Palacio19}. Among external measures for clustering validation, the most established one is the adjusted Rand Index \parencite{Hubert85}. It belongs to the class of binary external measures \parencite{Handl05}. Since there are more than one clustering result, the algorithm considers all sample pairs and computes the similarity between two sets of labels, based on the true and predicted labels for each item. The resulting values of the index lie within the range of -1.0 to 1.0 score, where the higher values mean a better clustering. The main advantage of the index is its interpretability and ease of use for all sort of clustering algorithms as long as true labels are available. 

Jaccard similarity score is a similar approach, where the intersection of two label sets divided by the number of all labels \parencite{Arnaboldi15}. However, since this metric rewards only positive label agreements \parencite{Handl05} it is not reliable enough in case of a lack of positive labels for some classes.

\subsection{Internal clustering validation methods}
Internal validity assesses the quality of clusters based only on the clustering information (assigned labels) and the information inherent to the data. The internal validation is based on two main criteria:
\begin{itemize}
  \item \emph{Compactness} (cohesion) deals with the intra-cluster distance and estimates the closeness of the elements within each cluster.
  \item \emph{Separation} addresses the inter-cluster distance and indicates how well-separated each cluster is from the other ones.
\end{itemize}
According to this definition a clustering algorithm is considered a good one if it has a small intra-cluster distance and large inter-cluster distance \parencite{Kim05}. Therefore, most clustering validation methods take into account both criteria during the assessment. Among them, the most popular validation measures proposed in the literature are Calinski-Harabasz index (\emph{CH}) \parencite{Calinski74}, Dunn's index \parencite{Dunn74}, Davies-Bouldin index (\emph{DB}) \parencite{DaviesB79}, Silhouette index \parencite{Rousseeuw87}. In order to find the optimal number of clusters the corresponding index should be maximized or as in the case of DB index minimized.

These measures, however, cannot adequately assess performance of a density-based clustering algorithms that identify arbitrarily shaped clusters, since these measures estimate separation based on the cluster centroids and are not able to handle the noise in the data \parencite{Moulavi14}. To address the assessment of algorithms based on density and hierarchical analysis, \textcite{Saracli13} tried to identify the optimal number of clusters using as an evaluation metric a Cophenetic Correlation Coefficient (\emph{CPCC}). This index, proposed by Sokal and Rohlf in \cite*{Sokal62}, uses a special distance measure, called  cophenetic distance, to compute a dendrogram of the hierarchical clustering. It shows the correlation between a cophenetic matrix which contains cophenetic distances between the points in the dendrogram and a distance (proximity) matrix which contains the information about the similarity of observations. Although this index is quite appropriate for hierarchical clustering \parencite{Palacio19}, it still underestimates the performance on arbitrarily shaped clusters. Another drawback is its high computational complexity.

Halkidi and Vazirgiannis in \cite*{Halkidi01b} have introduced the \emph{S\_Dbw} index (Scat-Density between and within clusters index) that was intended to find the best data partition by taking into account the density variations among clusters \parencite{Halkidi01b}. Compared to other clustering validation measures, \emph{S\_Dbw} index performs very well for different clustering algorithms including hierarchical one and DBSCAN \parencite{Liu10}. Nevertheless, since the separation between clusters is computed based on the centres of the clusters (centroids), this index cannot properly assess the performance on non-convex clusters.

In order to overcome this limitation, the same authors have proposed then another measure called \emph{CDbw} index (Composed density between and within cluster) \parencite{Halkidi08}. The crucial difference between \emph{CDbw} and \emph{S\_Dbw} validation measures is that in the former each cluster is represented not by a single point but by multi-representative points \parencite{Halkidi08}. Therefore, \emph{CDbw} index, based on the density distribution of points within and between clusters, can handle aspherical, arbitrarily shaped clusters. Nonetheless, this validity index also has some shortcomings. The main drawback of this metric is that when taking the number of representative points for each cluster to evaluate it, \emph{CDbw} index keep this number of points unchanged during the whole validation process. As a consequence, the cluster with a different density or number of objects cannot be evaluated correctly or even evaluated at all \parencite{Moulavi14}.

The new density-based clustering validation metric called DBCV was proposed by \textcite{Moulavi14} to overcome all the shortcomings of the previous measures by taking into account both the density and shapes of the clusters. In DBCV, the idea of the density computation is based on the Hartigan’s model of Density Contour Trees \parencite{Hartigan75}, where clusters are defined as areas with high density separated from each other by low-density areas \parencite{Moulavi14, Hartigan75}. Following this definition, the authors of the DBCV index assume that the least dense regions inside each cluster are still denser than the most dense areas between them \parencite{Moulavi14}. Therefore, for the evaluation of the compactness of the clusters they compute the least dense regions inside each cluster and in order to measure between-cluster separation they evaluate the highest density areas between these clusters. This definition of the object core distance makes DBCV index more capable of dealing with noise in the data and enables it to capture the performance on arbitrarily shaped clusters of different sizes and densities. The index output a score in the range [-1, 1], where the large index value means the better clustering.

\section{Summary}
This section briefly summarizes the methods mentioned above that are most relevant for this thesis, that is, embeddings, dimensionality reduction, and clustering. 

Several embedding techniques such as \verb|word2vec|, \verb|doc2vec| and Transformer-based sentence embedding methods are reviewed. For the experiments conducted as a part of this thesis, the following methods for getting a document and sentence embedding were chosen: the Paragraph Vector Model (\verb|doc2vec|), since it has an integrated document-unique feature vector, and a group of Transformer-based approaches, that aim to learn the sentence representations.

Among the topic modelling approaches, the mixed-membership probabilistic topic modelling (LDA) and single-membership model (Top2Vec) were discussed. In LDA, topics are seen as discrete probability distributions over words from the corpus and each topic has a set probability of occurring in any given document. Top2Vec, on the other hand, is an example of single-membership model, where the semantic vector space is regarded as a continuous representation of the topics. In the experimental part of this work, the LDA algorithm is used as a baseline topic model, since it is the most common method for identifying the underlying topics in large corpora. The Top2Vec method is used then for topic modelling and semantic search to discover the optimal clustering solution. 

Lastly, the techniques relevant to Top2Vec algorithm were covered. These included dimensionality reduction methods and hierarchical density-based cluster analysis, namely, UMAP, t-SNE, and HDBSCAN. While the first dimensionality reduction method, UMAP, is good at maintaining both the local and the global structure of the data and is scalable to larger datasets, t-SNE demonstrates more sensitivity to the local structure. Therefore, like in Top2Vec algorithm, during the experiments the dimensionality reduction of the full document representation are performed using UMAP algorithm, whereas t-SNE is used for topic cluster visualization.

When dealing with unsupervised clustering algorithms, the validation procedure is of great importance. Clustering validity methods can be divided into external and internal measures, depending on whether they estimate the quality of clustering using the external knowledge (the true class labels) or rely exclusively on the information intrinsic to the data. As topic modelling with Top2Vec at the moment of writing this thesis does not have any metric to verify the quality of the resulting topic clusters, two measures, external and internal, were chosen for the clustering validation. As an external validity index the adjusted Rand Score was applied to a small manually labeled subcorpus. The choice of the internal measure was based on the necessity to evaluate arbitrarily-shaped density-based clusters. Thus, for the internal validity assessment, the DBCV index was used.

\chapter{Dataset}
\label{chap:dataset}
This chapter aims at providing a representation of the data used to experiment with Top2Vec topic modelling algorithm and different embedding models\footnote{More details about embeddings model the Top2Vec algorithm was tested with please see in the next Chapter (Chapter \ref{chap:exp}).} to find the optimal algorithm parameters. In order to evaluate the clustering results in a supervised way, a small labelled corpus has been created. For unsupervised training, a much larger dataset was used. 

The overview of the entire dataset is presented in Section \ref{sec:corp_exploration} of this chapter. The process of preparation of the dataset for cluster evaluation in the supervised and unsupervised mode is described in Section \ref{sec:exp_setup}.

\section{Corpus description}
\label{sec:corp_exploration}

The dataset for this thesis was provided by the Ippen Digital GmbH \& Co. KG. It consists of German news articles from various regional newspaper publishers like Merkur, TZ, Frankfurter Rundschau, etc. The articles cover a broad range of topics from local news, business and politics to sport news, TV, and articles on the automotive industry.

The data were extracted from the in-house database for the period of January to April 2021. Each document from this database apart from the content also contains metadata such as a document ID, length (the number of characters in the article), "client" (what newspaper publisher the article comes from), "category", "headline", seo headline, along with some task-irrelevant information. An exemplary document is demonstrated in the Listing \ref{listing:json-example}.

To create a dataset, all documents within the mentioned period were filtered by the following categories: "Auto", "Games", "Politik" (Politics), "Fußball" (Football), "Sport", "Kultur" (Culture), "Serien" (Series), "Film, TV \& Serien", "Serien, TV \& Kino" (Series, TV \& Cinema). The last three categories were considered as belonging to one topic since they have similar content, but differ only in their name, depending on the newspaper they belong to. For clustering evaluation after training, this category was unified under the name of "Serien". The entire dataset consists of the 17.532 documents, that was extracted with all available metadata and saved in JSON files.

Before training, the content of the documents was pre-processed to remove the lines that have nothing to do with the article content, like HTML hyperlinks or irrelevant metadata (e.g., © dpa-infocom).


\begin{listing}[ht!]
\begin{minted}[frame=single,
            style=tango,
            showtabs=false,     
            framesep=3mm,
            escapeinside={\%*}{*)},        
            linenos=true,
            breaklines=true,
            xleftmargin=21pt,
            tabsize=4]{json}
{
    "doc_id": 90161825, 
    "length": 998, 
    "client": "Merkur.de", 
    "clientid": 268, 
    "category": "Politik", 
    "headline": "Impfbereitschaft in Deutschland nimmt zu", 
    "seo_headline": "Impfbereitschaft in Deutschland nimmt zu", 
    "publishing_date": "2021-01-07 19:01:18", 
    "content": " Berlin (dpa) - Die Bereitschaft zur Impfung gegen das Coronavirus hat laut einer Umfrage in Deutschland zugenommen. In einer Befragung von infratest dimap für den ARD-Deutschlandtrend von Anfang der Woche gaben 54 Prozent an, sich auf jeden Fall gegen das Coronavirus impfen lassen zu wollen. Das waren 17 Prozentpunkte mehr als im November 2020. Weitere 21 Prozent sagten, sie wollten sich wahrscheinlich impfen lassen (minus 13 Prozentpunkte). Die Impfbereitschaft sei in allen Altersgruppen gewachsen, insbesondere aber bei den Menschen unter 65 Jahren. Nach wie vor zeigten sich jüngere Menschen gegenüber einer Corona-Impfung weniger offen als ältere. Rund 36 Prozent der Befragten bezeichneten das Tempo der begonnenen Corona-Impfungen als angemessen, mehr als die Hälfte (52 Prozent) empfinden es als zu langsam. 70 Prozent erklärten, es sei richtig gewesen, dass die EU-Länder die Impfstoffe gemeinsam bestellt haben. 26 Prozent halten das für falsch. © dpa-infocom, dpa:210107-99-936480/2", 
    "image": 25393708, 
    "modification_date": "2021-01-07 19:06:30"
}

\end{minted}
\caption{The example of the document from the in-house database.} 
\label{listing:json-example}
\end{listing}
 
\section{Preparation of the dataset for supervised cluster evaluation}
\label{sec:exp_setup}

It is important to run Top2Vec algorithm on a small labeled section of the dataset, as the algorithm results depend on the embedding used, as well as on UMAP and HDBSCAN parameters. Having a labeled dataset allows to run the algorithm in a semi-supervised way and to measure the quality of the resulting clusters not only with an internal DBCV index \parencite{Moulavi14}, but also with an external one, namely, the adjusted Rand index \parencite{Hubert85}. 

For this purpose, a small labeled subcorpus was created, where only articles for March 2021 from the dataset were chosen and then filtered by the following categories: "Auto", "Games", "Fußball", "Serien" (and its name variations). These categories were selected in order to have articles with easily separable content, despite the fact that some words between at least three categories ("Games", "Fußball", "Serien") could be shared. Such a selection was meant to make the behaviour of the algorithm more transparent.

The final labeled subcorpus, created in this manner, consists of 1.640 documents with the following document distribution among the four categories:

\begin{table}[ht]
    \centering
    \begin{tabular}{ |c|c| } 
        \hline
        \textbf{Category} & \textbf{Number of documents} \\
        \hline
        Auto & 240 \\
        \hline
        Games & 254 \\
        \hline
        Fußball & 840 \\
        \hline
        Serien & 306 \\
        \hline
    \end{tabular}
    \caption{Document distribution within each of the four selected categories in the labeled subcorpus.}
    \label{tab:doc_dist}
\end{table}

Considering the fact that Top2Vec algorithm is prone to finding a large number of subtopics, it was expected to identify more than four clusters after training. Therefore, in order to measure the quality of these clusters, all articles were manually annotated. 

The annotation process was carried out in four steps. Firstly, the subtopics of each category were identified. Secondly, each document was assigned a label that corresponds to the subtopic of the document. Thirdly, a dictionary with the mapping of the corresponding subtopic (label) to the relevant keywords was created. Finally, all manually assigned labels were mapped to their corresponding IDs based on the matching between topic and label keywords (the same ID as that of the topic in case of match or a new ID). This was done because to compute the validity score the adjusted Rand index requires not class labels, but true and predicted topic IDs.

This labelled subcorpus was used for embedding and UMAP and HDBSCAN parameter selection. The set of parameters that worked best for the Top2Vec algorithm on the labelled subcorpus was used for working with the entire dataset.

\chapter{Experiments}
\label{chap:exp}

This chapter presents the experiments with the methods covered in Chapter \ref{chapter:theory} and some aspects of implementation. The baseline model is addressed in Section \ref{sec:lda_baseline}. The experiments are laid out in Section \ref{sec:exp}. They include, firstly, the tests with different embedding models, both the ones suggested by Top2Vec algorithm and the ones additionally integrated into it with the purpose of obtaining German embedding, (Section \ref{sec:exp_emb}), and, secondly, hyperparameter selection for improved clustering performance (Section \ref{sec:hypeparam}).

\section{Test with LDA as the baseline topic model}
\label{sec:lda_baseline}

Since topic modelling is an unsupervised learning method, it is a difficult to find the optimal heuristic for it. Therefore, a simple baseline model is needed to understand the interaction of the topic modelling algorithm with the dataset used as well as the possible challenges that should be taken into account when looking for a more complex model. Being the most commonly used topic modelling algorithm, Latent Dirichlet Allocation (LDA) covered in Section \ref{sec:lda} of Chapter \ref{chapter:theory} was chosen as the baseline model.

The LDA algorithm was run on the small labelled dataset with 200 iterations\footnote{The number of iterations was determined based on the log likelihood parameter, according to which 200 iteratins was enough for the convergence of the LDA model.}. Since documents in this dataset belong to four clearly separated categories, the total number of topics to be obtained (parameter \(K\)) was set to 4. The alpha (0.5) parameter was set to a small value, as the resulting clusters were expected to be quite homogeneous. The necessary pre-processing, such as tokenization, lemmatization and stemming, as well as stop-words removal, was performed before training.

\renewcommand{\arraystretch}{2}
\begin{table}[ht]
    \centering
    \begin{tabularx}{\textwidth}{ |c|c|X| } 
        \hline
        \textbf{Topic Id} & \textbf{Freq} & \textbf{Topic Keywords} \\
        \hline
        1 & 0.34720 &  löw, sommer, bundestrainer, trainer, spieler, dfb, deutschland, fc bayern, fußball, münchen \\
        \hline
        2 & 0.25652 & werder bremen, saison, tor, mannschaft, sieg, sv werder bremen, ball, trainer, partie, werder \\
        \hline
        3 & 0.24969 & auto, netflix, staffel, adac, serie, prozent, autofahrer, deutschland, fahrzeug, sony \\
        \hline
        4 & 0.14658 &  love island, staffel, adriano, folge, bianca, script, rtl, emilia, kandidaten, liebe \\
        \hline
    \end{tabularx}
    \caption{LDA Topics with topic score and topic keywords.}
    \label{tab:lda_topic_keywords}
\end{table}
\renewcommand{\arraystretch}{1}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{LDA_1.png}
\caption{pyLDAvis visualization of identified topics with the word distribution for the \(1^{st}\) topic.}
\label{fig:lda_topics}
\end{figure}

Table \ref{tab:lda_topic_keywords} shows the resulting LDA topics and their keywords. It is clear from the keywords, that the first two topics are very similar semantically. Both of them cover the football topic and are, in fact, its subtopics. Furthermore, as Figure \ref{fig:lda_topics} shows, these subtopics overlap. As for the third and the fourth topics, although they are well-separated, as demonstrated in Figure \ref{fig:lda_topics}, they are not very well-defined. It is not clear from the keywords of the third topic, whether it is a Series or a Football cluster. The fourth topic seems to represent a Series cluster, while the Games theme is not presented at all.

However, it is worth mentioning at this point that the theme distribution in the dataset (both the small subcorpus and the complete dataset) is rather imbalanced. As already shown in Table \ref{tab:doc_dist}, the Football category has the highest number of articles compared to the other three categories in the subcorpus. However, since the real-world data are often imbalanced, it could be seen as a challenge that a topic modelling algorithm has to deal with.

\section{Experiments with various embedding techniques for Top2Vec}
\label{sec:exp}

This section presents all the experiments performed with Top2Vec topic modelling algorithm conducted on German data. Firstly, the experiments on the small labelled subcorpus were run in order to determine the optimal hyperparameter set. Here, two validity indices described in Chapter \ref{chapter:theory} (Subsection \ref{sec:validation}) were used for clustering evaluation: Density-Based Clustering Validation index (DBCV) \parencite{Moulavi14}, which is an internal validation metric, and the adjusted Rand Index \parencite{Hubert85}, which is an external one. Then, Top2Vec algorithm with the parameter set that has achieved the best results per each embedding model was run on the entire dataset. For the lack of available ground truth labels, the resulting clusters were evaluated only with the internal DBCV index.

Please note, that the clustering validity index alone is not enough to access the validity of clusters and an additional instrument like visualization should be used \parencite{Halkidi01b} to verify the resulting scores. To assess how homogeneous the resulting clusters are and to check the assignment of topics to documents an HTML file was created after each model training. The file contained both the original information from the in-house database and the model output. The original information included such metadata as document ID, client, category, headline, and content. The output of the trained model consisted of the topic ID and topic keywords assigned to each document. The Listing \ref{listing:data-example} demonstrates the same document as the Listing \ref{listing:json-example} saved after training with some original metadata and the information provided by the Top2Vec model.

\begin{listing}[ht]
\begin{minted}[frame=single,
            style=borland,
            showtabs=false,     
            framesep=3mm,
            escapeinside={\%*}{*)},        
            linenos=true,
            breaklines=true,
            xleftmargin=21pt,
            tabsize=4]{html}
<tr>
  <td>90161825</td>
  <td>tz</td>
  <td>Politik</td>
  <td>Impfbereitschaft in Deutschland nimmt zu</td>
  <td>Berlin (dpa) - Die Bereitschaft zur Impfung gegen das Coronavirus hat laut einer Umfrage in Deutschland zugenommen. In einer Befragung von infratest dimap für den ARD-Deutschlandtrend von Anfang der Woche gaben 54 Prozent an, sich auf jeden Fall gegen das Coronavirus impfen lassen zu wollen. Das waren 17 Prozentpunkte mehr als im November 2020. Weitere 21 Prozent sagten, sie wollten sich wahrscheinlich impfen lassen (minus 13 Prozentpunkte). Die Impfbereitschaft sei in allen Altersgruppen gewachsen, insbesondere aber bei den Menschen unter 65 Jahren. Nach wie vor zeigten sich jüngere Menschen gegenüber einer Corona-Impfung weniger offen als ältere. Rund 36 Prozent der Befragten bezeichneten das Tempo der begonnenen Corona-Impfungen als angemessen, mehr als die Hälfte (52 Prozent) empfinden es als zu langsam. 70 Prozent erklärten, es sei richtig gewesen, dass die EU-Länder die Impfstoffe gemeinsam bestellt haben. 26 Prozent halten das für falsch.</td>
  <td>53</td>
  <td>[forsa, prozentpunkte, umfrage, prozentpunkt, insa, befragten, prozentpunkten, erhebung, prozent, umfragen, umfragewerte, bundestagswahl, infratest, dimap, bundesburger, grunen, auftrag, unionsparteien, unverandert, kanzlerkandidat, befragung, minus, wahlern, befragt, yougov, fdp, wahler, kanzlerschaft, wahlen, kame, annalena, baerbock, habeck, landtagswahlen, unions, ampelkoalition, cdu, laschet, zweitstarkste, ergab, kanzler, zustimmung, zuwachs, spd, armin, gleichauf, union, vergleich, regierende, wahlberechtigte]</td>
</tr>
\end{minted}
\caption{The same document with metadata and the information from the trained model (topic ID and topic keywords).} 
\label{listing:data-example}
\end{listing}

The resulting clusters were visualized\footnote{For the clusters visualization the t-SNE dimensionality reduction algorithma and interactive data visualization Python library \href{https://bokeh.org/}{Bokeh} was used.} using all this information, such that each data point, i.e. a document, got an annotation label that contained the ID of the corresponding document, its headline, as well as the assigned topic ID and topic keywords. Figure \ref{fig:cluster_vis} shows an example of the cluster visualization for one of the trained models. The topic clusters themselves are plotted by the combination of different colours and shapes of the points, the legend with the cluster IDs is shown on the right side. During the analysing of topic clusters, an interactive label annotation with the document ID, headline, topic ID, and topic keywords was used for every data point.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{vis_example.png}
\caption{The example of cluster visualization with t-SNE.}
\label{fig:cluster_vis}
\end{figure}


\subsection{Experiments with embedding models}
\label{sec:exp_emb}

During the experiments, the Top2Vec algorithm was tested with Paragraph Vectors Framework (\verb|doc2vec|) and several Transformer-based models that make it possible to get a document and sentence embedding for the German language.

The \verb|doc2vec| model was trained from scratch within the Top2Vec model with 400\footnote{Both on the small labelled dataset (subcorpus) and the entire unlabelled dataset.} epochs and the other doc2vec parameters were determined in the Top2Vec topic modelling algorithm. For training, from the two versions of \verb|doc2vec| described in Section \ref{sec:doc2vec} the Paragraph Vector with Distributed Bag of Words (DBOW) was chosen. The number of dimensions on the feature vector was set to 300 and the window size was equal to 15. The \verb|dbow_words| parameter was set to 1 resulting in the jointly embedded document and words since their vectors were learned parallel.

The following four Transformer-based embedding models were used with pre-trained embeddings. Two of these models, multilingual Universal Sentence Encoder\footnote{https://tfhub.dev/google/universal-sentence-encoder-multilingual/3} and "Distiluse-base-multilingual-cased"\footnote{https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/}, are already built into the Top2Vec algorithm and provide a multilingual embedding. The other two models were additionally integrated into Top2Vec to obtain a German embedding.

The multilingual Universal Sentence Encoder embedding model was trained on various texts from 16 languages including English, German, French, Arabic, and Chinese among others. Its training data consisted of question-answering pairs from online forums and QA websites\footnote{Reddit, StackOverflow, and YahooAnswers.}, translation pairs, and the Stanford Natural Language Inference (SNLI) corpus \parencite{Cer18c}. Since the SNLI corpus contains only English sentences, to obtain the data in other 15 languages, the sentences were translated with Google's translation system \parencite{Cer18c}. The resulting training corpus amounted to 8 million samples of sentences.

The embedding models described below are based on Sentence-Bert (SBERT) architecture \parencite{Reimers19}. They were pre-trained on the parallel data from nine different datasets\footnote{Such datasets include GlobalVoices corpus of news stories, News-Commentary dataset of political and economic commentaries, WikiMatrix of parallel sentences from Wikipedia, as well as other translated texts collections.} using multilingual knowledge distillation approach \parencite{Reimers20}. 

The "Distiluse-base-multilingual-cased" pre-trained model was fine-tuned on multilingual semantic textual similarity (STS) data and supports more than 50 languages. 

The remaining two embedding models were not used in the original Top2Vec algorithm but were additionally integrated into it to obtain a German embedding. The first model is German RoBERTa model for Sentence Embeddings\footnote{The model can be found under the name: \href{https://huggingface.co/T-Systems-onsite/german-roberta-sentence-transformer-v2}{\textquote{T-Systems-onsite/german-roberta-sentence-transformer-v2}}} and the second one is bilingual model for English and German\footnote{The model can be found under the name: \href{https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer}{\textquote{T-Systems-onsite/cross-en-de-roberta-sentence-transformer}}}. Both of them are based on XLM-RoBERTa (XML-R) model, which was trained for the cross-lingual task on more than 50 Million paraphrases pairs, most of which were derived from Wikipedia \parencite{Reimers20}.

\subsection{UMAP and HDBSCAN hyperparameter optimization}
\label{sec:hypeparam}

Both UMAP and HDBSCAN algorithms have a lot of hyperparameters that must be optimized. However, only few of them have a strong impact on the resulting clusters. The Top2Vec algorithm was tested with varying values of these parameters to find an optimal clustering solution. For the UMAP dimensionality reduction algorithm, the most important hyperparameters are the number of nearest neighbours (\verb|n_neighbors|), \verb|n_components| that defines the dimensionality in the reduced dimension space, and the distance metric used to compute the distance or dissimilarity between two data points.For the HDBSCAN algorithm, the clustering result mostly depends on such hyperparameters as the minimal size of clusters (\verb|min_cluster_size|) and the number of samples in a neighbourhood needed to regard a particular point as a core one (\verb|min_samples|). Tables \ref{tab:UMAP_param} and \ref{tab:HDBSCAN_param} illustrate the search space for hyperparameter optimization. 

\begin{table}[ht]
    \centering
    \begin{tabular}{ |c|c|c| } 
        \hline
        \multicolumn{3}{|c|}{\textbf{UMAP}} \\
        \hline\hline
        \textbf{Hyperparameter} & \textbf{Search space} &  \textbf{Default in Top2Vec} \\
        \hline
        \verb|n_neighbors| & [10, 15, 25] & 15 \\
        \hline
        \verb|n_components| & [2, 3, 5, 7, 10] & 5 \\
        \hline
        \verb|metric| & ['cosine', 'euclidean'] &  'cosine' \\
        \hline
    \end{tabular}
    \caption{Hyperparameter optimization for UMAP.}
    \label{tab:UMAP_param}
\end{table}


\begin{table}[ht]
    \centering
    \begin{tabular}{ |c|c|c| } 
        \hline
        \multicolumn{3}{|c|}{\textbf{HDBSCAN}} \\
        \hline\hline
        \textbf{Hyperparameter} & \textbf{Search space} &  \textbf{Default in Top2Vec} \\
        \hline
        \verb|min_cluster_size| & [5, 10, 15] & 15 \\
        \hline
        \verb|min_samples| & [None, 1] & None \\
        \hline
    \end{tabular}
    \caption{Hyperparameter optimization for HDBSCAN.}
    \label{tab:HDBSCAN_param}
\end{table}

During the experiments, the topic clusters of each model trained on the small labelled dataset were evaluated with two metrics: first, with the internal DBCV measure to understand how compact and well separated they are, and secondly, using the labels, with the external metric, adjusted RandIndex (ARI). Both DBCV and ARI metric has resulting scores that lie in the range [-1, 1], where a higher value means a better clustering result.

To verify to what extent the global structure of topic clusters is preserved, the topics discovered by the Top2Vec model were condensed to the number of the original categories (4) through the hierarchical topic reduction. The resulting clusters were then evaluated with the ARI metric. In order to assess the model in terms of local data structure preservation, i.e. how many subtopics were identified, the original clusters of the model were evaluated with ARI metric without any topic reduction procedure. Subsequently, the average score of two ARI measures (for global and local structure) was computed. 

After that, the three best models were chosen. If all three best ARI scores belonged to different models, firstly, the model with the highest average ARI score was chosen and secondly, that of with the best ARI score for the preserving of the local structure, since for this task good identification of subtopics is more important than the maintaining of the global structure of data. After that, the model with the best DBCV index was considered. Then, the topics identified by these three best models within each of the four categories were evaluated with ARI metric.

The procedure of hyperparameter optimization was repeated for every embedding model described in the previous section. Subsequently, the Top2Vec algorithm was run on the whole dataset with the three sets of UMAP and HDBSCAN hyperparameters that achieved the highest ARIs scores and a good DBCV index. Additionally, for every embedding model, the Top2Vec algorithm was also run with its default UMAP and HDBSCAN parameters. The topic clusters obtained from the experiments on the entire dataset were evaluated only with the DBCV metric.



\chapter{Results and evaluation}

blablabla

\section{Top2Vec with (doc2vec) embedding}
\label{sec:doc2vec_results}
blabal

\section{Top2Vec with Multilingual universal sentence encoder}
\label{sec:muse_results}

blablabla

\section{Top2Vec with Sentence Transformers}
\label{sec:senttrans_results}
blablabla


\subsection{Distiluse-base-multilingual-cased}
\label{sec:muse_result}
blabal


\subsection{German RoBERTa for Sentence Embeddings V2}
\label{sec:roberta_de_result}
blabal

\subsection{Cross English \& German RoBERTa for Sentence Embeddings}
\label{sec:roberta_de_en_result}
blabal

\section{Discussion}
\label{sec:discussion}
blablabla


\chapter{Conclusion and Future Work}
blablabla

\section{Conclusion}
blablabla

\section{Future Work}
blablabla


\newpage



% Imports the bibliography file "sample.bib" 
%Includes "Bibliography" in the table of contents
\printbibliography[heading=bibintoc]


\newpage

% Abbildungsverzeichnis (kann auch nach dem Inhaltsverzeichnis kommen)
\listoffigures
\newpage

% Tabellenverzeichnis (kann auch nach dem Inhaltsverzeichnis kommen)
\listoftables
\newpage

% \lstlistoflistings
% % \addcontentsline{toc}{section}{Listings}
% \newpage


\addchap{Inhalt der beigelegten CD}

\end{document}

